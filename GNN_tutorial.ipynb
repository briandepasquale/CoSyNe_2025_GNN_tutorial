{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWeMasEycv3v",
        "outputId": "9c9d935e-64ab-48db-b493-44e5e9bad81c"
      },
      "outputs": [],
      "source": [
        "#!pip install numpy\n",
        "#!pip install scipy\n",
        "#!pip install torch\n",
        "#!pip install torch_geometric\n",
        "#!pip install matplotlib\n",
        "#!pip install scikit-learn\n",
        "#!pip install pandas\n",
        "#!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!pip freeze > requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1op-CbyLuN4",
        "outputId": "14c55aa5-9ef8-4375-a55e-2f14923432e6"
      },
      "outputs": [],
      "source": [
        "# Install required packages.\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_homophily(edge_index, y):\n",
        "    # Ensure edge_index is undirected\n",
        "    row, col = edge_index\n",
        "    same_label = (y[row] == y[col])\n",
        "    return same_label.sum().item() / edge_index.size(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "graph = torch.load('SMP_R_graph.pt', weights_only=False)\n",
        "#graph = torch.load('GNG_graph.pt', weights_only=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Homophily: 0.9326\n"
          ]
        }
      ],
      "source": [
        "homophily = compute_homophily(graph.edge_index, graph.y)\n",
        "print(f\"Homophily: {homophily:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num_classes = len(graph.y.unique())\n",
        "num_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_features = graph.x.shape[1]\n",
        "num_nodes = graph.x.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute class weights\n",
        "label_counts = torch.bincount(graph.y[graph.train_mask])\n",
        "class_weights = 1.0 / label_counts.float()\n",
        "class_weights = class_weights / class_weights.sum()  # normalize if you want"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 500x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from torch_geometric.utils import subgraph, to_networkx\n",
        "import networkx as nx\n",
        "\n",
        "# Draw subgraph\n",
        "plt.figure(figsize=(5, 5))\n",
        "#nx.draw(G, with_labels=True, node_color=\"lightblue\", edge_color=\"gray\", node_size=100, font_size=8)\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.title(\"Subgraph Visualization\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OWGw54wRd98"
      },
      "source": [
        "## Training a Graph Neural Network (GNN)\n",
        "\n",
        "We can easily convert our MLP to a GNN by swapping the `torch.nn.Linear` layers with PyG's GNN operators.\n",
        "\n",
        "Following-up on [the first part of this tutorial](https://colab.research.google.com/drive/1h3-vJGRVloF5zStxL5I0rSy4ZUPNsjy8), we replace the linear layers by the [`GCNConv`](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GCNConv) module.\n",
        "To recap, the **GCN layer** ([Kipf et al. (2017)](https://arxiv.org/abs/1609.02907)) is defined as\n",
        "\n",
        "$$\n",
        "\\mathbf{x}_v^{(\\ell + 1)} = \\mathbf{W}^{(\\ell + 1)} \\sum_{w \\in \\mathcal{N}(v) \\, \\cup \\, \\{ v \\}} \\frac{1}{c_{w,v}} \\cdot \\mathbf{x}_w^{(\\ell)}\n",
        "$$\n",
        "\n",
        "where $\\mathbf{W}^{(\\ell + 1)}$ denotes a trainable weight matrix of shape `[num_output_features, num_input_features]` and $c_{w,v}$ refers to a fixed normalization coefficient for each edge.\n",
        "In contrast, a single `Linear` layer is defined as\n",
        "\n",
        "$$\n",
        "\\mathbf{x}_v^{(\\ell + 1)} = \\mathbf{W}^{(\\ell + 1)} \\mathbf{x}_v^{(\\ell)}\n",
        "$$\n",
        "\n",
        "which does not make use of neighboring node information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmXWs1dKIzD8",
        "outputId": "e64121b1-5996-4805-df6b-3d03942622eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GCN(\n",
            "  (conv1): GCNConv(6, 16)\n",
            "  (conv2): GCNConv(16, 6)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super().__init__()\n",
        "        torch.manual_seed(1234567)\n",
        "        self.conv1 = GCNConv(num_features, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, num_classes)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = F.dropout(x, p=0.5, training=self.training)  # Dropout only active during training\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "model = GCN(hidden_channels=16)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "p3TAi69zI1bO",
        "outputId": "ae10e725-23c3-4231-f256-9346464cc08e"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 001, Loss: 1.7816, val_error: 0.5383\n",
            "Epoch: 002, Loss: 1.7822, val_error: 0.6253\n",
            "Epoch: 003, Loss: 1.8085, val_error: 0.5280\n",
            "Epoch: 004, Loss: 1.7932, val_error: 0.4887\n",
            "Epoch: 005, Loss: 1.7917, val_error: 0.4501\n",
            "Epoch: 006, Loss: 1.7743, val_error: 0.4048\n",
            "Epoch: 007, Loss: 1.7606, val_error: 0.3548\n",
            "Epoch: 008, Loss: 1.7464, val_error: 0.3119\n",
            "Epoch: 009, Loss: 1.7360, val_error: 0.3178\n",
            "Epoch: 010, Loss: 1.7419, val_error: 0.3083\n",
            "Epoch: 011, Loss: 1.7337, val_error: 0.3067\n",
            "Epoch: 012, Loss: 1.7335, val_error: 0.2332\n",
            "Epoch: 013, Loss: 1.7430, val_error: 0.2626\n",
            "Epoch: 014, Loss: 1.7378, val_error: 0.2567\n",
            "Epoch: 015, Loss: 1.7194, val_error: 0.2630\n",
            "Epoch: 016, Loss: 1.7201, val_error: 0.2710\n",
            "Epoch: 017, Loss: 1.7117, val_error: 0.2749\n",
            "Epoch: 018, Loss: 1.7131, val_error: 0.2765\n",
            "Epoch: 019, Loss: 1.6748, val_error: 0.2833\n",
            "Epoch: 020, Loss: 1.6916, val_error: 0.2813\n",
            "Epoch: 021, Loss: 1.7191, val_error: 0.2896\n",
            "Epoch: 022, Loss: 1.6938, val_error: 0.2841\n",
            "Epoch: 023, Loss: 1.6722, val_error: 0.2737\n",
            "Epoch: 024, Loss: 1.6899, val_error: 0.2471\n",
            "Epoch: 025, Loss: 1.6773, val_error: 0.2447\n",
            "Epoch: 026, Loss: 1.6887, val_error: 0.2427\n",
            "Epoch: 027, Loss: 1.6756, val_error: 0.2368\n",
            "Epoch: 028, Loss: 1.6487, val_error: 0.2094\n",
            "Epoch: 029, Loss: 1.6342, val_error: 0.1903\n",
            "Epoch: 030, Loss: 1.6430, val_error: 0.1943\n",
            "Epoch: 031, Loss: 1.6043, val_error: 0.1875\n",
            "Epoch: 032, Loss: 1.6127, val_error: 0.1681\n",
            "Epoch: 033, Loss: 1.6342, val_error: 0.1601\n",
            "Epoch: 034, Loss: 1.6078, val_error: 0.1581\n",
            "Epoch: 035, Loss: 1.6263, val_error: 0.1534\n",
            "Epoch: 036, Loss: 1.6049, val_error: 0.1422\n",
            "Epoch: 037, Loss: 1.6149, val_error: 0.1597\n",
            "Epoch: 038, Loss: 1.6045, val_error: 0.1685\n",
            "Epoch: 039, Loss: 1.6278, val_error: 0.1685\n",
            "Epoch: 040, Loss: 1.5882, val_error: 0.1943\n",
            "Epoch: 041, Loss: 1.5777, val_error: 0.2086\n",
            "Epoch: 042, Loss: 1.5556, val_error: 0.1935\n",
            "Epoch: 043, Loss: 1.5467, val_error: 0.1979\n",
            "Epoch: 044, Loss: 1.5708, val_error: 0.1593\n",
            "Epoch: 045, Loss: 1.5938, val_error: 0.1621\n",
            "Epoch: 046, Loss: 1.5186, val_error: 0.1613\n",
            "Epoch: 047, Loss: 1.4999, val_error: 0.1625\n",
            "Epoch: 048, Loss: 1.5286, val_error: 0.1641\n",
            "Epoch: 049, Loss: 1.5307, val_error: 0.1553\n",
            "Epoch: 050, Loss: 1.5100, val_error: 0.1728\n",
            "Epoch: 051, Loss: 1.5367, val_error: 0.1764\n",
            "Epoch: 052, Loss: 1.4987, val_error: 0.1597\n",
            "Epoch: 053, Loss: 1.5432, val_error: 0.1685\n",
            "Epoch: 054, Loss: 1.5946, val_error: 0.1716\n",
            "Epoch: 055, Loss: 1.5023, val_error: 0.1808\n",
            "Epoch: 056, Loss: 1.4804, val_error: 0.1589\n",
            "Epoch: 057, Loss: 1.5018, val_error: 0.1538\n",
            "Epoch: 058, Loss: 1.5498, val_error: 0.1641\n",
            "Epoch: 059, Loss: 1.4363, val_error: 0.1248\n",
            "Epoch: 060, Loss: 1.4554, val_error: 0.1212\n",
            "Epoch: 061, Loss: 1.5620, val_error: 0.1140\n",
            "Epoch: 062, Loss: 1.4361, val_error: 0.1049\n",
            "Epoch: 063, Loss: 1.4062, val_error: 0.1097\n",
            "Epoch: 064, Loss: 1.4519, val_error: 0.1176\n",
            "Epoch: 065, Loss: 1.4281, val_error: 0.1208\n",
            "Epoch: 066, Loss: 1.4002, val_error: 0.1212\n",
            "Epoch: 067, Loss: 1.4460, val_error: 0.1144\n",
            "Epoch: 068, Loss: 1.5169, val_error: 0.1236\n",
            "Epoch: 069, Loss: 1.4178, val_error: 0.1442\n",
            "Epoch: 070, Loss: 1.4181, val_error: 0.1442\n",
            "Epoch: 071, Loss: 1.4773, val_error: 0.1605\n",
            "Epoch: 072, Loss: 1.4397, val_error: 0.1617\n",
            "Epoch: 073, Loss: 1.4140, val_error: 0.1808\n",
            "Epoch: 074, Loss: 1.4449, val_error: 0.1780\n",
            "Epoch: 075, Loss: 1.4533, val_error: 0.1573\n",
            "Epoch: 076, Loss: 1.3987, val_error: 0.1669\n",
            "Epoch: 077, Loss: 1.4311, val_error: 0.1534\n",
            "Epoch: 078, Loss: 1.4199, val_error: 0.1387\n",
            "Epoch: 079, Loss: 1.4762, val_error: 0.1351\n",
            "Epoch: 080, Loss: 1.3562, val_error: 0.1339\n",
            "Epoch: 081, Loss: 1.3502, val_error: 0.1251\n",
            "Epoch: 082, Loss: 1.2792, val_error: 0.1363\n",
            "Epoch: 083, Loss: 1.3282, val_error: 0.1240\n",
            "Epoch: 084, Loss: 1.3771, val_error: 0.1279\n",
            "Epoch: 085, Loss: 1.3404, val_error: 0.1224\n",
            "Epoch: 086, Loss: 1.3472, val_error: 0.1323\n",
            "Epoch: 087, Loss: 1.3082, val_error: 0.1379\n",
            "Epoch: 088, Loss: 1.3642, val_error: 0.1446\n",
            "Epoch: 089, Loss: 1.4348, val_error: 0.1581\n",
            "Epoch: 090, Loss: 1.3790, val_error: 0.1565\n",
            "Epoch: 091, Loss: 1.3382, val_error: 0.1685\n",
            "Epoch: 092, Loss: 1.3486, val_error: 0.1903\n",
            "Epoch: 093, Loss: 1.4029, val_error: 0.1951\n",
            "Epoch: 094, Loss: 1.3041, val_error: 0.1891\n",
            "Epoch: 095, Loss: 1.3071, val_error: 0.1780\n",
            "Epoch: 096, Loss: 1.3641, val_error: 0.1919\n",
            "Epoch: 097, Loss: 1.3175, val_error: 0.1712\n",
            "Epoch: 098, Loss: 1.3582, val_error: 0.1637\n",
            "Epoch: 099, Loss: 1.2687, val_error: 0.1585\n",
            "Epoch: 100, Loss: 1.2470, val_error: 0.1577\n",
            "Epoch: 101, Loss: 1.2827, val_error: 0.1681\n",
            "Epoch: 102, Loss: 1.3944, val_error: 0.1577\n",
            "Epoch: 103, Loss: 1.3925, val_error: 0.1510\n",
            "Epoch: 104, Loss: 1.3381, val_error: 0.1398\n",
            "Epoch: 105, Loss: 1.2947, val_error: 0.1522\n",
            "Epoch: 106, Loss: 1.3031, val_error: 0.1589\n",
            "Epoch: 107, Loss: 1.2522, val_error: 0.1279\n",
            "Epoch: 108, Loss: 1.2930, val_error: 0.1633\n",
            "Epoch: 109, Loss: 1.2802, val_error: 0.1426\n",
            "Epoch: 110, Loss: 1.3354, val_error: 0.1947\n",
            "Epoch: 111, Loss: 1.2786, val_error: 0.1657\n",
            "Epoch: 112, Loss: 1.2823, val_error: 0.1617\n",
            "Epoch: 113, Loss: 1.2845, val_error: 0.1820\n",
            "Epoch: 114, Loss: 1.3410, val_error: 0.1836\n",
            "Epoch: 115, Loss: 1.2211, val_error: 0.1740\n",
            "Epoch: 116, Loss: 1.3443, val_error: 0.1959\n",
            "Epoch: 117, Loss: 1.3251, val_error: 0.1792\n",
            "Epoch: 118, Loss: 1.2706, val_error: 0.1708\n",
            "Epoch: 119, Loss: 1.2393, val_error: 0.1871\n",
            "Epoch: 120, Loss: 1.3145, val_error: 0.1804\n",
            "Epoch: 121, Loss: 1.3396, val_error: 0.1820\n",
            "Epoch: 122, Loss: 1.2983, val_error: 0.1839\n",
            "Epoch: 123, Loss: 1.2706, val_error: 0.2006\n",
            "Epoch: 124, Loss: 1.1944, val_error: 0.2050\n",
            "Epoch: 125, Loss: 1.2387, val_error: 0.2126\n",
            "Epoch: 126, Loss: 1.2469, val_error: 0.2102\n",
            "Epoch: 127, Loss: 1.1859, val_error: 0.1967\n",
            "Epoch: 128, Loss: 1.2729, val_error: 0.2110\n",
            "Epoch: 129, Loss: 1.1877, val_error: 0.2054\n",
            "Epoch: 130, Loss: 1.2565, val_error: 0.2002\n",
            "Epoch: 131, Loss: 1.2853, val_error: 0.1947\n",
            "Epoch: 132, Loss: 1.2860, val_error: 0.1899\n",
            "Epoch: 133, Loss: 1.1912, val_error: 0.1720\n",
            "Epoch: 134, Loss: 1.2256, val_error: 0.1788\n",
            "Epoch: 135, Loss: 1.2011, val_error: 0.1788\n",
            "Epoch: 136, Loss: 1.2299, val_error: 0.1792\n",
            "Epoch: 137, Loss: 1.2635, val_error: 0.1883\n",
            "Epoch: 138, Loss: 1.2464, val_error: 0.1947\n",
            "Epoch: 139, Loss: 1.2510, val_error: 0.1875\n",
            "Epoch: 140, Loss: 1.2404, val_error: 0.1979\n",
            "Epoch: 141, Loss: 1.2384, val_error: 0.1907\n",
            "Epoch: 142, Loss: 1.2311, val_error: 0.2137\n",
            "Epoch: 143, Loss: 1.2553, val_error: 0.2201\n",
            "Epoch: 144, Loss: 1.2444, val_error: 0.2114\n",
            "Epoch: 145, Loss: 1.2036, val_error: 0.2157\n",
            "Epoch: 146, Loss: 1.2067, val_error: 0.2098\n",
            "Epoch: 147, Loss: 1.2780, val_error: 0.2280\n",
            "Epoch: 148, Loss: 1.2374, val_error: 0.2368\n",
            "Epoch: 149, Loss: 1.2014, val_error: 0.2118\n",
            "Epoch: 150, Loss: 1.2493, val_error: 0.2161\n",
            "Epoch: 151, Loss: 1.1455, val_error: 0.2050\n",
            "Epoch: 152, Loss: 1.1263, val_error: 0.2062\n",
            "Epoch: 153, Loss: 1.2487, val_error: 0.1828\n",
            "Epoch: 154, Loss: 1.2105, val_error: 0.1875\n",
            "Epoch: 155, Loss: 1.2404, val_error: 0.2042\n",
            "Epoch: 156, Loss: 1.1341, val_error: 0.1931\n",
            "Epoch: 157, Loss: 1.2013, val_error: 0.2165\n",
            "Epoch: 158, Loss: 1.2967, val_error: 0.2265\n",
            "Epoch: 159, Loss: 1.2425, val_error: 0.2324\n",
            "Epoch: 160, Loss: 1.1430, val_error: 0.2237\n",
            "Epoch: 161, Loss: 1.1691, val_error: 0.2269\n",
            "Epoch: 162, Loss: 1.1724, val_error: 0.2137\n",
            "Epoch: 163, Loss: 1.1078, val_error: 0.2086\n",
            "Epoch: 164, Loss: 1.2006, val_error: 0.1804\n",
            "Epoch: 165, Loss: 1.2323, val_error: 0.1712\n",
            "Epoch: 166, Loss: 1.1519, val_error: 0.1645\n",
            "Epoch: 167, Loss: 1.2076, val_error: 0.1756\n",
            "Epoch: 168, Loss: 1.1289, val_error: 0.1792\n",
            "Epoch: 169, Loss: 1.1123, val_error: 0.1911\n",
            "Epoch: 170, Loss: 1.0905, val_error: 0.1760\n",
            "Epoch: 171, Loss: 1.1708, val_error: 0.2042\n",
            "Epoch: 172, Loss: 1.1220, val_error: 0.1939\n",
            "Epoch: 173, Loss: 1.1224, val_error: 0.2086\n",
            "Epoch: 174, Loss: 1.1762, val_error: 0.2277\n",
            "Epoch: 175, Loss: 1.2244, val_error: 0.2209\n",
            "Epoch: 176, Loss: 1.3047, val_error: 0.2424\n",
            "Epoch: 177, Loss: 1.2268, val_error: 0.2213\n",
            "Epoch: 178, Loss: 1.1456, val_error: 0.2237\n",
            "Epoch: 179, Loss: 1.1193, val_error: 0.2110\n",
            "Epoch: 180, Loss: 1.1017, val_error: 0.1986\n",
            "Epoch: 181, Loss: 1.0752, val_error: 0.1986\n",
            "Epoch: 182, Loss: 1.1798, val_error: 0.1899\n",
            "Epoch: 183, Loss: 1.2035, val_error: 0.2050\n",
            "Epoch: 184, Loss: 1.1935, val_error: 0.2261\n",
            "Epoch: 185, Loss: 1.1477, val_error: 0.2340\n",
            "Epoch: 186, Loss: 1.1222, val_error: 0.2523\n",
            "Epoch: 187, Loss: 1.0748, val_error: 0.2646\n",
            "Epoch: 188, Loss: 1.1994, val_error: 0.2698\n",
            "Epoch: 189, Loss: 1.1292, val_error: 0.2523\n",
            "Epoch: 190, Loss: 1.1405, val_error: 0.2745\n",
            "Epoch: 191, Loss: 1.3220, val_error: 0.2439\n",
            "Epoch: 192, Loss: 1.1417, val_error: 0.2578\n",
            "Epoch: 193, Loss: 1.1177, val_error: 0.2122\n",
            "Epoch: 194, Loss: 1.2428, val_error: 0.2141\n",
            "Epoch: 195, Loss: 1.0950, val_error: 0.2086\n",
            "Epoch: 196, Loss: 1.2499, val_error: 0.2046\n",
            "Epoch: 197, Loss: 1.2127, val_error: 0.1867\n",
            "Epoch: 198, Loss: 1.1583, val_error: 0.2130\n",
            "Epoch: 199, Loss: 1.1331, val_error: 0.2249\n",
            "Epoch: 200, Loss: 1.1101, val_error: 0.2181\n",
            "Epoch: 201, Loss: 1.1071, val_error: 0.2153\n",
            "Epoch: 202, Loss: 1.1721, val_error: 0.2169\n",
            "Epoch: 203, Loss: 1.2509, val_error: 0.2288\n",
            "Epoch: 204, Loss: 1.1676, val_error: 0.2253\n",
            "Epoch: 205, Loss: 1.1177, val_error: 0.2189\n",
            "Epoch: 206, Loss: 1.1389, val_error: 0.2304\n",
            "Epoch: 207, Loss: 1.1361, val_error: 0.2424\n",
            "Epoch: 208, Loss: 1.1150, val_error: 0.2408\n",
            "Epoch: 209, Loss: 1.0501, val_error: 0.2447\n",
            "Epoch: 210, Loss: 1.1311, val_error: 0.2332\n",
            "Epoch: 211, Loss: 1.1148, val_error: 0.2443\n",
            "Epoch: 212, Loss: 1.1140, val_error: 0.2499\n",
            "Epoch: 213, Loss: 1.1111, val_error: 0.2491\n",
            "Epoch: 214, Loss: 1.1121, val_error: 0.2416\n",
            "Epoch: 215, Loss: 1.1229, val_error: 0.2551\n",
            "Epoch: 216, Loss: 1.1340, val_error: 0.2396\n",
            "Epoch: 217, Loss: 1.0617, val_error: 0.2356\n",
            "Epoch: 218, Loss: 1.1218, val_error: 0.2451\n",
            "Epoch: 219, Loss: 1.1118, val_error: 0.2229\n",
            "Epoch: 220, Loss: 1.1066, val_error: 0.2253\n",
            "Epoch: 221, Loss: 1.1549, val_error: 0.2161\n",
            "Epoch: 222, Loss: 1.1133, val_error: 0.2221\n",
            "Epoch: 223, Loss: 1.1444, val_error: 0.2193\n",
            "Epoch: 224, Loss: 1.0270, val_error: 0.2431\n",
            "Epoch: 225, Loss: 1.0349, val_error: 0.2459\n",
            "Epoch: 226, Loss: 1.0404, val_error: 0.2698\n",
            "Epoch: 227, Loss: 1.1273, val_error: 0.2531\n",
            "Epoch: 228, Loss: 1.0573, val_error: 0.2606\n",
            "Epoch: 229, Loss: 1.1002, val_error: 0.2507\n",
            "Epoch: 230, Loss: 1.0659, val_error: 0.2698\n",
            "Epoch: 231, Loss: 1.0722, val_error: 0.2698\n",
            "Epoch: 232, Loss: 1.1341, val_error: 0.2880\n",
            "Epoch: 233, Loss: 1.0496, val_error: 0.2865\n",
            "Epoch: 234, Loss: 1.1015, val_error: 0.2670\n",
            "Epoch: 235, Loss: 1.0722, val_error: 0.2900\n",
            "Epoch: 236, Loss: 1.1185, val_error: 0.2853\n",
            "Epoch: 237, Loss: 1.0849, val_error: 0.2666\n",
            "Epoch: 238, Loss: 1.1366, val_error: 0.2511\n",
            "Epoch: 239, Loss: 1.0509, val_error: 0.2515\n",
            "Epoch: 240, Loss: 1.0837, val_error: 0.2551\n",
            "Epoch: 241, Loss: 1.2351, val_error: 0.2598\n",
            "Epoch: 242, Loss: 1.1342, val_error: 0.2745\n",
            "Epoch: 243, Loss: 1.0445, val_error: 0.2694\n",
            "Epoch: 244, Loss: 1.0208, val_error: 0.2876\n",
            "Epoch: 245, Loss: 1.0327, val_error: 0.2789\n",
            "Epoch: 246, Loss: 1.0442, val_error: 0.3004\n",
            "Epoch: 247, Loss: 1.0782, val_error: 0.3111\n",
            "Epoch: 248, Loss: 1.0697, val_error: 0.2868\n",
            "Epoch: 249, Loss: 1.1109, val_error: 0.2729\n",
            "Epoch: 250, Loss: 1.0542, val_error: 0.2809\n",
            "Epoch: 251, Loss: 1.1297, val_error: 0.2833\n",
            "Epoch: 252, Loss: 1.0750, val_error: 0.2495\n",
            "Epoch: 253, Loss: 1.0688, val_error: 0.2408\n",
            "Epoch: 254, Loss: 1.0317, val_error: 0.2424\n",
            "Epoch: 255, Loss: 1.1035, val_error: 0.2479\n",
            "Epoch: 256, Loss: 1.1138, val_error: 0.2424\n",
            "Epoch: 257, Loss: 1.0476, val_error: 0.2404\n",
            "Epoch: 258, Loss: 0.9970, val_error: 0.2427\n",
            "Epoch: 259, Loss: 1.0684, val_error: 0.2578\n",
            "Epoch: 260, Loss: 1.0547, val_error: 0.2646\n",
            "Epoch: 261, Loss: 1.0951, val_error: 0.2849\n",
            "Epoch: 262, Loss: 1.1215, val_error: 0.2710\n",
            "Epoch: 263, Loss: 1.0703, val_error: 0.2769\n",
            "Epoch: 264, Loss: 1.0235, val_error: 0.2888\n",
            "Epoch: 265, Loss: 1.1178, val_error: 0.2952\n",
            "Epoch: 266, Loss: 1.1392, val_error: 0.2801\n",
            "Epoch: 267, Loss: 1.0766, val_error: 0.3143\n",
            "Epoch: 268, Loss: 1.1240, val_error: 0.2868\n",
            "Epoch: 269, Loss: 0.9811, val_error: 0.3004\n",
            "Epoch: 270, Loss: 1.1135, val_error: 0.2845\n",
            "Epoch: 271, Loss: 1.0104, val_error: 0.2841\n",
            "Epoch: 272, Loss: 1.0978, val_error: 0.2646\n",
            "Epoch: 273, Loss: 1.0746, val_error: 0.2495\n",
            "Epoch: 274, Loss: 1.0383, val_error: 0.2535\n",
            "Epoch: 275, Loss: 0.9914, val_error: 0.2543\n",
            "Epoch: 276, Loss: 1.0813, val_error: 0.2491\n",
            "Epoch: 277, Loss: 1.1154, val_error: 0.2626\n",
            "Epoch: 278, Loss: 1.0898, val_error: 0.2503\n",
            "Epoch: 279, Loss: 1.0934, val_error: 0.2694\n",
            "Epoch: 280, Loss: 1.0534, val_error: 0.2634\n",
            "Epoch: 281, Loss: 1.0768, val_error: 0.2924\n",
            "Epoch: 282, Loss: 1.1036, val_error: 0.2805\n",
            "Epoch: 283, Loss: 1.0092, val_error: 0.3067\n",
            "Epoch: 284, Loss: 1.1122, val_error: 0.3087\n",
            "Epoch: 285, Loss: 1.0216, val_error: 0.2884\n",
            "Epoch: 286, Loss: 1.0060, val_error: 0.3409\n",
            "Epoch: 287, Loss: 1.0462, val_error: 0.2984\n",
            "Epoch: 288, Loss: 0.9990, val_error: 0.2984\n",
            "Epoch: 289, Loss: 1.0683, val_error: 0.3023\n",
            "Epoch: 290, Loss: 1.0711, val_error: 0.2952\n",
            "Epoch: 291, Loss: 0.9928, val_error: 0.2809\n",
            "Epoch: 292, Loss: 0.9955, val_error: 0.2837\n",
            "Epoch: 293, Loss: 1.0405, val_error: 0.2745\n",
            "Epoch: 294, Loss: 1.0193, val_error: 0.2857\n",
            "Epoch: 295, Loss: 1.1226, val_error: 0.2988\n",
            "Epoch: 296, Loss: 1.0249, val_error: 0.3143\n",
            "Epoch: 297, Loss: 1.0480, val_error: 0.2872\n",
            "Epoch: 298, Loss: 1.1061, val_error: 0.3067\n",
            "Epoch: 299, Loss: 1.1459, val_error: 0.3206\n",
            "Epoch: 300, Loss: 1.0689, val_error: 0.2968\n",
            "Epoch: 301, Loss: 1.0173, val_error: 0.2972\n",
            "Epoch: 302, Loss: 0.9579, val_error: 0.3043\n",
            "Epoch: 303, Loss: 1.1163, val_error: 0.2964\n",
            "Epoch: 304, Loss: 1.1219, val_error: 0.2904\n",
            "Epoch: 305, Loss: 1.0669, val_error: 0.2992\n",
            "Epoch: 306, Loss: 1.0035, val_error: 0.3019\n",
            "Epoch: 307, Loss: 1.0263, val_error: 0.2868\n",
            "Epoch: 308, Loss: 1.0006, val_error: 0.2698\n",
            "Epoch: 309, Loss: 1.0271, val_error: 0.2817\n",
            "Epoch: 310, Loss: 1.0300, val_error: 0.2876\n",
            "Epoch: 311, Loss: 0.9672, val_error: 0.2642\n",
            "Epoch: 312, Loss: 1.0176, val_error: 0.2749\n",
            "Epoch: 313, Loss: 1.0867, val_error: 0.2896\n",
            "Epoch: 314, Loss: 1.0270, val_error: 0.2769\n",
            "Epoch: 315, Loss: 1.0082, val_error: 0.2968\n",
            "Epoch: 316, Loss: 1.0746, val_error: 0.2936\n",
            "Epoch: 317, Loss: 0.9833, val_error: 0.3302\n",
            "Epoch: 318, Loss: 1.0137, val_error: 0.2793\n",
            "Epoch: 319, Loss: 1.2077, val_error: 0.2865\n",
            "Epoch: 320, Loss: 0.9956, val_error: 0.3004\n",
            "Epoch: 321, Loss: 0.9948, val_error: 0.2837\n",
            "Epoch: 322, Loss: 1.0611, val_error: 0.3047\n",
            "Epoch: 323, Loss: 1.1096, val_error: 0.2968\n",
            "Epoch: 324, Loss: 0.9818, val_error: 0.3099\n",
            "Epoch: 325, Loss: 0.9906, val_error: 0.2972\n",
            "Epoch: 326, Loss: 1.0612, val_error: 0.2996\n",
            "Epoch: 327, Loss: 1.0980, val_error: 0.3103\n",
            "Epoch: 328, Loss: 0.9934, val_error: 0.3413\n",
            "Epoch: 329, Loss: 1.0277, val_error: 0.3484\n",
            "Epoch: 330, Loss: 1.1043, val_error: 0.3214\n",
            "Epoch: 331, Loss: 1.0218, val_error: 0.3357\n",
            "Epoch: 332, Loss: 0.9427, val_error: 0.3242\n",
            "Epoch: 333, Loss: 1.0151, val_error: 0.3023\n",
            "Epoch: 334, Loss: 1.0027, val_error: 0.3190\n",
            "Epoch: 335, Loss: 1.0135, val_error: 0.2745\n",
            "Epoch: 336, Loss: 0.9601, val_error: 0.2884\n",
            "Epoch: 337, Loss: 1.0312, val_error: 0.2841\n",
            "Epoch: 338, Loss: 1.0278, val_error: 0.2857\n",
            "Epoch: 339, Loss: 1.1608, val_error: 0.2857\n",
            "Epoch: 340, Loss: 1.0170, val_error: 0.2928\n",
            "Epoch: 341, Loss: 0.9887, val_error: 0.2916\n",
            "Epoch: 342, Loss: 1.1035, val_error: 0.2872\n",
            "Epoch: 343, Loss: 0.9649, val_error: 0.3095\n",
            "Epoch: 344, Loss: 1.0598, val_error: 0.3306\n",
            "Epoch: 345, Loss: 0.9406, val_error: 0.3409\n",
            "Epoch: 346, Loss: 1.0338, val_error: 0.3488\n",
            "Epoch: 347, Loss: 0.9799, val_error: 0.3472\n",
            "Epoch: 348, Loss: 1.0331, val_error: 0.3707\n",
            "Epoch: 349, Loss: 1.0506, val_error: 0.3492\n",
            "Epoch: 350, Loss: 1.0351, val_error: 0.3568\n",
            "Epoch: 351, Loss: 0.9019, val_error: 0.3445\n",
            "Epoch: 352, Loss: 1.0624, val_error: 0.3329\n",
            "Epoch: 353, Loss: 1.0809, val_error: 0.3174\n",
            "Epoch: 354, Loss: 1.0298, val_error: 0.3242\n",
            "Epoch: 355, Loss: 0.9872, val_error: 0.2960\n",
            "Epoch: 356, Loss: 1.0512, val_error: 0.2892\n",
            "Epoch: 357, Loss: 1.0563, val_error: 0.2868\n",
            "Epoch: 358, Loss: 1.0206, val_error: 0.2916\n",
            "Epoch: 359, Loss: 1.0285, val_error: 0.3023\n",
            "Epoch: 360, Loss: 1.0434, val_error: 0.3166\n",
            "Epoch: 361, Loss: 0.9764, val_error: 0.3083\n",
            "Epoch: 362, Loss: 1.0259, val_error: 0.3186\n",
            "Epoch: 363, Loss: 1.0550, val_error: 0.3278\n",
            "Epoch: 364, Loss: 1.0488, val_error: 0.3508\n",
            "Epoch: 365, Loss: 1.0036, val_error: 0.3290\n",
            "Epoch: 366, Loss: 1.0205, val_error: 0.3206\n",
            "Epoch: 367, Loss: 1.0739, val_error: 0.3401\n",
            "Epoch: 368, Loss: 1.0189, val_error: 0.3576\n",
            "Epoch: 369, Loss: 0.9942, val_error: 0.3377\n",
            "Epoch: 370, Loss: 0.9920, val_error: 0.3302\n",
            "Epoch: 371, Loss: 0.9763, val_error: 0.3258\n",
            "Epoch: 372, Loss: 1.0439, val_error: 0.3377\n",
            "Epoch: 373, Loss: 0.9791, val_error: 0.3127\n",
            "Epoch: 374, Loss: 0.9969, val_error: 0.3282\n",
            "Epoch: 375, Loss: 0.9730, val_error: 0.3317\n",
            "Epoch: 376, Loss: 0.9541, val_error: 0.3357\n",
            "Epoch: 377, Loss: 0.9791, val_error: 0.3516\n",
            "Epoch: 378, Loss: 1.0763, val_error: 0.3647\n",
            "Epoch: 379, Loss: 1.0507, val_error: 0.3512\n",
            "Epoch: 380, Loss: 1.0295, val_error: 0.3580\n",
            "Epoch: 381, Loss: 1.0618, val_error: 0.3500\n",
            "Epoch: 382, Loss: 0.8845, val_error: 0.3389\n",
            "Epoch: 383, Loss: 0.9275, val_error: 0.3480\n",
            "Epoch: 384, Loss: 0.9749, val_error: 0.3254\n",
            "Epoch: 385, Loss: 0.9675, val_error: 0.3298\n",
            "Epoch: 386, Loss: 1.0379, val_error: 0.3456\n",
            "Epoch: 387, Loss: 0.9969, val_error: 0.3242\n",
            "Epoch: 388, Loss: 1.0486, val_error: 0.3246\n",
            "Epoch: 389, Loss: 0.9964, val_error: 0.3182\n",
            "Epoch: 390, Loss: 0.9918, val_error: 0.3278\n",
            "Epoch: 391, Loss: 0.9743, val_error: 0.3381\n",
            "Epoch: 392, Loss: 1.0664, val_error: 0.3548\n",
            "Epoch: 393, Loss: 0.9005, val_error: 0.3278\n",
            "Epoch: 394, Loss: 1.0210, val_error: 0.3345\n",
            "Epoch: 395, Loss: 1.0524, val_error: 0.3484\n",
            "Epoch: 396, Loss: 0.9586, val_error: 0.3294\n",
            "Epoch: 397, Loss: 1.0497, val_error: 0.3353\n",
            "Epoch: 398, Loss: 1.0606, val_error: 0.3313\n",
            "Epoch: 399, Loss: 0.9311, val_error: 0.3274\n",
            "Epoch: 400, Loss: 0.9475, val_error: 0.3389\n",
            "Epoch: 401, Loss: 1.0039, val_error: 0.3401\n",
            "Epoch: 402, Loss: 0.9005, val_error: 0.3588\n",
            "Epoch: 403, Loss: 0.9462, val_error: 0.3596\n",
            "Epoch: 404, Loss: 0.9712, val_error: 0.3556\n",
            "Epoch: 405, Loss: 0.9638, val_error: 0.3723\n",
            "Epoch: 406, Loss: 0.9734, val_error: 0.3492\n",
            "Epoch: 407, Loss: 0.9550, val_error: 0.3484\n",
            "Epoch: 408, Loss: 0.9377, val_error: 0.3647\n",
            "Epoch: 409, Loss: 1.0412, val_error: 0.3647\n",
            "Epoch: 410, Loss: 0.9014, val_error: 0.3536\n",
            "Epoch: 411, Loss: 0.9836, val_error: 0.3306\n",
            "Epoch: 412, Loss: 0.9518, val_error: 0.3572\n",
            "Epoch: 413, Loss: 0.9900, val_error: 0.3504\n",
            "Epoch: 414, Loss: 0.9359, val_error: 0.3357\n",
            "Epoch: 415, Loss: 0.9760, val_error: 0.3349\n",
            "Epoch: 416, Loss: 0.8815, val_error: 0.3373\n",
            "Epoch: 417, Loss: 1.0143, val_error: 0.3548\n",
            "Epoch: 418, Loss: 1.0597, val_error: 0.3671\n",
            "Epoch: 419, Loss: 1.0288, val_error: 0.3405\n",
            "Epoch: 420, Loss: 1.0054, val_error: 0.3401\n",
            "Epoch: 421, Loss: 0.9989, val_error: 0.3556\n",
            "Epoch: 422, Loss: 0.9711, val_error: 0.3802\n",
            "Epoch: 423, Loss: 0.9708, val_error: 0.3671\n",
            "Epoch: 424, Loss: 0.9909, val_error: 0.3790\n",
            "Epoch: 425, Loss: 0.9512, val_error: 0.3627\n",
            "Epoch: 426, Loss: 0.9811, val_error: 0.3993\n",
            "Epoch: 427, Loss: 0.8879, val_error: 0.3735\n",
            "Epoch: 428, Loss: 0.9733, val_error: 0.3532\n",
            "Epoch: 429, Loss: 1.0149, val_error: 0.3747\n",
            "Epoch: 430, Loss: 0.9907, val_error: 0.3631\n",
            "Epoch: 431, Loss: 0.9367, val_error: 0.3735\n",
            "Epoch: 432, Loss: 0.9337, val_error: 0.3981\n",
            "Epoch: 433, Loss: 0.9454, val_error: 0.3711\n",
            "Epoch: 434, Loss: 0.9524, val_error: 0.3739\n",
            "Epoch: 435, Loss: 0.9654, val_error: 0.3790\n",
            "Epoch: 436, Loss: 1.0311, val_error: 0.3544\n",
            "Epoch: 437, Loss: 0.9318, val_error: 0.3786\n",
            "Epoch: 438, Loss: 0.8871, val_error: 0.3576\n",
            "Epoch: 439, Loss: 0.9200, val_error: 0.3536\n",
            "Epoch: 440, Loss: 0.9330, val_error: 0.3512\n",
            "Epoch: 441, Loss: 0.9319, val_error: 0.3584\n",
            "Epoch: 442, Loss: 1.0016, val_error: 0.3754\n",
            "Epoch: 443, Loss: 1.0115, val_error: 0.3691\n",
            "Epoch: 444, Loss: 1.0083, val_error: 0.3937\n",
            "Epoch: 445, Loss: 0.8854, val_error: 0.3770\n",
            "Epoch: 446, Loss: 0.8990, val_error: 0.3866\n",
            "Epoch: 447, Loss: 0.9189, val_error: 0.3735\n",
            "Epoch: 448, Loss: 0.9089, val_error: 0.3750\n",
            "Epoch: 449, Loss: 1.0252, val_error: 0.3758\n",
            "Epoch: 450, Loss: 0.9799, val_error: 0.3476\n",
            "Epoch: 451, Loss: 0.9075, val_error: 0.3564\n",
            "Epoch: 452, Loss: 1.0134, val_error: 0.3707\n",
            "Epoch: 453, Loss: 0.9654, val_error: 0.3623\n",
            "Epoch: 454, Loss: 0.9785, val_error: 0.3735\n",
            "Epoch: 455, Loss: 1.0143, val_error: 0.3683\n",
            "Epoch: 456, Loss: 0.9986, val_error: 0.3544\n",
            "Epoch: 457, Loss: 0.9375, val_error: 0.3687\n",
            "Epoch: 458, Loss: 0.9484, val_error: 0.3556\n",
            "Epoch: 459, Loss: 0.9677, val_error: 0.3603\n",
            "Epoch: 460, Loss: 0.9504, val_error: 0.3834\n",
            "Epoch: 461, Loss: 0.9150, val_error: 0.3945\n",
            "Epoch: 462, Loss: 1.0604, val_error: 0.4033\n",
            "Epoch: 463, Loss: 0.9440, val_error: 0.3921\n",
            "Epoch: 464, Loss: 1.0256, val_error: 0.3997\n",
            "Epoch: 465, Loss: 0.9959, val_error: 0.3671\n",
            "Epoch: 466, Loss: 0.9924, val_error: 0.3675\n",
            "Epoch: 467, Loss: 0.9957, val_error: 0.3536\n",
            "Epoch: 468, Loss: 1.0394, val_error: 0.3699\n",
            "Epoch: 469, Loss: 0.9483, val_error: 0.3651\n",
            "Epoch: 470, Loss: 0.9715, val_error: 0.3866\n",
            "Epoch: 471, Loss: 0.9976, val_error: 0.3600\n",
            "Epoch: 472, Loss: 1.0203, val_error: 0.3675\n",
            "Epoch: 473, Loss: 0.9358, val_error: 0.3691\n",
            "Epoch: 474, Loss: 0.9401, val_error: 0.3476\n",
            "Epoch: 475, Loss: 0.9677, val_error: 0.3683\n",
            "Epoch: 476, Loss: 0.9407, val_error: 0.3691\n",
            "Epoch: 477, Loss: 0.9576, val_error: 0.3842\n",
            "Epoch: 478, Loss: 0.8949, val_error: 0.3917\n",
            "Epoch: 479, Loss: 0.9630, val_error: 0.4068\n",
            "Epoch: 480, Loss: 0.9504, val_error: 0.4124\n",
            "Epoch: 481, Loss: 0.9257, val_error: 0.4259\n",
            "Epoch: 482, Loss: 0.9677, val_error: 0.4247\n",
            "Epoch: 483, Loss: 0.9468, val_error: 0.3949\n",
            "Epoch: 484, Loss: 0.9555, val_error: 0.3993\n",
            "Epoch: 485, Loss: 0.9573, val_error: 0.4076\n",
            "Epoch: 486, Loss: 0.9788, val_error: 0.4116\n",
            "Epoch: 487, Loss: 1.0038, val_error: 0.3830\n",
            "Epoch: 488, Loss: 0.9172, val_error: 0.3806\n",
            "Epoch: 489, Loss: 0.9795, val_error: 0.3842\n",
            "Epoch: 490, Loss: 1.0121, val_error: 0.3524\n",
            "Epoch: 491, Loss: 0.9541, val_error: 0.3667\n",
            "Epoch: 492, Loss: 0.9274, val_error: 0.3619\n",
            "Epoch: 493, Loss: 0.9826, val_error: 0.4005\n",
            "Epoch: 494, Loss: 1.0583, val_error: 0.3790\n",
            "Epoch: 495, Loss: 0.9387, val_error: 0.3750\n",
            "Epoch: 496, Loss: 0.9151, val_error: 0.3969\n",
            "Epoch: 497, Loss: 0.9439, val_error: 0.4068\n",
            "Epoch: 498, Loss: 0.9795, val_error: 0.4080\n",
            "Epoch: 499, Loss: 1.0063, val_error: 0.3921\n",
            "Epoch: 500, Loss: 0.9220, val_error: 0.4033\n",
            "Epoch: 501, Loss: 0.9647, val_error: 0.4136\n",
            "Epoch: 502, Loss: 0.9580, val_error: 0.4009\n",
            "Epoch: 503, Loss: 0.8956, val_error: 0.4152\n",
            "Epoch: 504, Loss: 0.9970, val_error: 0.3957\n",
            "Epoch: 505, Loss: 0.9428, val_error: 0.4017\n",
            "Epoch: 506, Loss: 0.9990, val_error: 0.3965\n",
            "Epoch: 507, Loss: 1.0036, val_error: 0.3826\n",
            "Epoch: 508, Loss: 0.9440, val_error: 0.3854\n",
            "Epoch: 509, Loss: 0.8739, val_error: 0.3818\n",
            "Epoch: 510, Loss: 1.0360, val_error: 0.4148\n",
            "Epoch: 511, Loss: 0.8787, val_error: 0.4005\n",
            "Epoch: 512, Loss: 0.9968, val_error: 0.3989\n",
            "Epoch: 513, Loss: 0.8947, val_error: 0.3961\n",
            "Epoch: 514, Loss: 0.9581, val_error: 0.3810\n",
            "Epoch: 515, Loss: 0.9642, val_error: 0.3921\n",
            "Epoch: 516, Loss: 1.2686, val_error: 0.3878\n",
            "Epoch: 517, Loss: 0.9420, val_error: 0.4096\n",
            "Epoch: 518, Loss: 0.8555, val_error: 0.4120\n",
            "Epoch: 519, Loss: 0.8577, val_error: 0.4283\n",
            "Epoch: 520, Loss: 1.0988, val_error: 0.3810\n",
            "Epoch: 521, Loss: 0.9167, val_error: 0.3917\n",
            "Epoch: 522, Loss: 0.8554, val_error: 0.4279\n",
            "Epoch: 523, Loss: 0.9666, val_error: 0.4033\n",
            "Epoch: 524, Loss: 0.9068, val_error: 0.4342\n",
            "Epoch: 525, Loss: 0.8778, val_error: 0.4338\n",
            "Epoch: 526, Loss: 0.8868, val_error: 0.4219\n",
            "Epoch: 527, Loss: 0.9173, val_error: 0.4251\n",
            "Epoch: 528, Loss: 1.0083, val_error: 0.4076\n",
            "Epoch: 529, Loss: 0.8979, val_error: 0.4505\n",
            "Epoch: 530, Loss: 0.8798, val_error: 0.4438\n",
            "Epoch: 531, Loss: 0.9293, val_error: 0.4124\n",
            "Epoch: 532, Loss: 1.1292, val_error: 0.4275\n",
            "Epoch: 533, Loss: 1.0132, val_error: 0.4219\n",
            "Epoch: 534, Loss: 0.9432, val_error: 0.4160\n",
            "Epoch: 535, Loss: 0.9296, val_error: 0.4088\n",
            "Epoch: 536, Loss: 0.9645, val_error: 0.4184\n",
            "Epoch: 537, Loss: 0.9078, val_error: 0.4378\n",
            "Epoch: 538, Loss: 0.9297, val_error: 0.4430\n",
            "Epoch: 539, Loss: 0.9039, val_error: 0.4235\n",
            "Epoch: 540, Loss: 0.9607, val_error: 0.4458\n",
            "Epoch: 541, Loss: 0.8357, val_error: 0.4354\n",
            "Epoch: 542, Loss: 0.9074, val_error: 0.4764\n",
            "Epoch: 543, Loss: 0.8981, val_error: 0.4648\n",
            "Epoch: 544, Loss: 1.0200, val_error: 0.4597\n",
            "Epoch: 545, Loss: 0.8831, val_error: 0.4950\n",
            "Epoch: 546, Loss: 0.9540, val_error: 0.4648\n",
            "Epoch: 547, Loss: 0.9821, val_error: 0.4390\n",
            "Epoch: 548, Loss: 0.8872, val_error: 0.4176\n",
            "Epoch: 549, Loss: 0.9758, val_error: 0.4231\n",
            "Epoch: 550, Loss: 0.9852, val_error: 0.4239\n",
            "Epoch: 551, Loss: 0.9556, val_error: 0.4263\n",
            "Epoch: 552, Loss: 1.0426, val_error: 0.4323\n",
            "Epoch: 553, Loss: 0.9661, val_error: 0.4291\n",
            "Epoch: 554, Loss: 1.0096, val_error: 0.4263\n",
            "Epoch: 555, Loss: 0.9721, val_error: 0.4164\n",
            "Epoch: 556, Loss: 0.9278, val_error: 0.4295\n",
            "Epoch: 557, Loss: 0.8520, val_error: 0.4303\n",
            "Epoch: 558, Loss: 0.9228, val_error: 0.4470\n",
            "Epoch: 559, Loss: 0.9932, val_error: 0.4434\n",
            "Epoch: 560, Loss: 0.9481, val_error: 0.4422\n",
            "Epoch: 561, Loss: 0.8923, val_error: 0.4311\n",
            "Epoch: 562, Loss: 0.9761, val_error: 0.4569\n",
            "Epoch: 563, Loss: 0.9221, val_error: 0.4362\n",
            "Epoch: 564, Loss: 1.0121, val_error: 0.4370\n",
            "Epoch: 565, Loss: 0.9648, val_error: 0.4358\n",
            "Epoch: 566, Loss: 0.9136, val_error: 0.4263\n",
            "Epoch: 567, Loss: 0.8715, val_error: 0.4227\n",
            "Epoch: 568, Loss: 0.9387, val_error: 0.4199\n",
            "Epoch: 569, Loss: 0.9846, val_error: 0.4136\n",
            "Epoch: 570, Loss: 0.9406, val_error: 0.4346\n",
            "Epoch: 571, Loss: 0.9064, val_error: 0.4434\n",
            "Epoch: 572, Loss: 0.9470, val_error: 0.4378\n",
            "Epoch: 573, Loss: 0.9367, val_error: 0.4569\n",
            "Epoch: 574, Loss: 0.9807, val_error: 0.4307\n",
            "Epoch: 575, Loss: 1.0737, val_error: 0.4398\n",
            "Epoch: 576, Loss: 0.9747, val_error: 0.4664\n",
            "Epoch: 577, Loss: 0.9553, val_error: 0.4847\n",
            "Epoch: 578, Loss: 0.9696, val_error: 0.4672\n",
            "Epoch: 579, Loss: 0.8621, val_error: 0.4632\n",
            "Epoch: 580, Loss: 0.9664, val_error: 0.4672\n",
            "Epoch: 581, Loss: 1.1464, val_error: 0.4557\n",
            "Epoch: 582, Loss: 0.9524, val_error: 0.4080\n",
            "Epoch: 583, Loss: 0.9773, val_error: 0.4398\n",
            "Epoch: 584, Loss: 0.9848, val_error: 0.4184\n",
            "Epoch: 585, Loss: 0.8724, val_error: 0.4068\n",
            "Epoch: 586, Loss: 0.8881, val_error: 0.4184\n",
            "Epoch: 587, Loss: 0.9090, val_error: 0.3965\n",
            "Epoch: 588, Loss: 0.9458, val_error: 0.4060\n",
            "Epoch: 589, Loss: 0.8696, val_error: 0.4152\n",
            "Epoch: 590, Loss: 0.8917, val_error: 0.4386\n",
            "Epoch: 591, Loss: 0.9408, val_error: 0.4390\n",
            "Epoch: 592, Loss: 0.8512, val_error: 0.4311\n",
            "Epoch: 593, Loss: 0.8519, val_error: 0.4124\n",
            "Epoch: 594, Loss: 1.0651, val_error: 0.4529\n",
            "Epoch: 595, Loss: 1.0363, val_error: 0.4315\n",
            "Epoch: 596, Loss: 1.0023, val_error: 0.4279\n",
            "Epoch: 597, Loss: 0.9383, val_error: 0.4438\n",
            "Epoch: 598, Loss: 0.9547, val_error: 0.4382\n",
            "Epoch: 599, Loss: 0.9314, val_error: 0.4323\n",
            "Epoch: 600, Loss: 0.9017, val_error: 0.4315\n",
            "Epoch: 601, Loss: 0.9133, val_error: 0.4180\n",
            "Epoch: 602, Loss: 0.9293, val_error: 0.4291\n",
            "Epoch: 603, Loss: 0.9442, val_error: 0.4243\n",
            "Epoch: 604, Loss: 0.8821, val_error: 0.4307\n",
            "Epoch: 605, Loss: 0.8942, val_error: 0.4632\n",
            "Epoch: 606, Loss: 0.9128, val_error: 0.4553\n",
            "Epoch: 607, Loss: 0.8799, val_error: 0.4664\n",
            "Epoch: 608, Loss: 0.9970, val_error: 0.4636\n",
            "Epoch: 609, Loss: 0.9256, val_error: 0.5002\n",
            "Epoch: 610, Loss: 0.9476, val_error: 0.4795\n",
            "Epoch: 611, Loss: 0.9457, val_error: 0.4712\n",
            "Epoch: 612, Loss: 0.9092, val_error: 0.4764\n",
            "Epoch: 613, Loss: 0.8853, val_error: 0.4593\n",
            "Epoch: 614, Loss: 0.9042, val_error: 0.4668\n",
            "Epoch: 615, Loss: 0.9398, val_error: 0.4370\n",
            "Epoch: 616, Loss: 1.0455, val_error: 0.4474\n",
            "Epoch: 617, Loss: 0.9189, val_error: 0.4346\n",
            "Epoch: 618, Loss: 0.9133, val_error: 0.4410\n",
            "Epoch: 619, Loss: 0.9528, val_error: 0.4466\n",
            "Epoch: 620, Loss: 0.8785, val_error: 0.4342\n",
            "Epoch: 621, Loss: 0.8859, val_error: 0.4513\n",
            "Epoch: 622, Loss: 0.9474, val_error: 0.4597\n",
            "Epoch: 623, Loss: 0.9466, val_error: 0.4629\n",
            "Epoch: 624, Loss: 0.8765, val_error: 0.4482\n",
            "Epoch: 625, Loss: 0.9231, val_error: 0.4807\n",
            "Epoch: 626, Loss: 1.0859, val_error: 0.4672\n",
            "Epoch: 627, Loss: 0.9051, val_error: 0.4899\n",
            "Epoch: 628, Loss: 0.9295, val_error: 0.4736\n",
            "Epoch: 629, Loss: 0.8920, val_error: 0.4744\n",
            "Epoch: 630, Loss: 0.9391, val_error: 0.4962\n",
            "Epoch: 631, Loss: 0.8476, val_error: 0.4752\n",
            "Epoch: 632, Loss: 0.9309, val_error: 0.4585\n",
            "Epoch: 633, Loss: 0.9378, val_error: 0.4799\n",
            "Epoch: 634, Loss: 0.9462, val_error: 0.4827\n",
            "Epoch: 635, Loss: 0.9626, val_error: 0.4811\n",
            "Epoch: 636, Loss: 0.9278, val_error: 0.4692\n",
            "Epoch: 637, Loss: 0.9376, val_error: 0.4879\n",
            "Epoch: 638, Loss: 0.9781, val_error: 0.4779\n",
            "Epoch: 639, Loss: 0.9201, val_error: 0.4644\n",
            "Epoch: 640, Loss: 0.9009, val_error: 0.4732\n",
            "Epoch: 641, Loss: 0.9595, val_error: 0.4712\n",
            "Epoch: 642, Loss: 0.9482, val_error: 0.4553\n",
            "Epoch: 643, Loss: 0.9913, val_error: 0.4664\n",
            "Epoch: 644, Loss: 0.9388, val_error: 0.4390\n",
            "Epoch: 645, Loss: 0.9312, val_error: 0.4505\n",
            "Epoch: 646, Loss: 0.9062, val_error: 0.4454\n",
            "Epoch: 647, Loss: 0.9062, val_error: 0.4402\n",
            "Epoch: 648, Loss: 0.8939, val_error: 0.4378\n",
            "Epoch: 649, Loss: 0.9395, val_error: 0.4644\n",
            "Epoch: 650, Loss: 0.9238, val_error: 0.4398\n",
            "Epoch: 651, Loss: 0.8778, val_error: 0.4748\n",
            "Epoch: 652, Loss: 0.9953, val_error: 0.4764\n",
            "Epoch: 653, Loss: 0.8870, val_error: 0.4815\n",
            "Epoch: 654, Loss: 0.9064, val_error: 0.4855\n",
            "Epoch: 655, Loss: 0.8844, val_error: 0.4811\n",
            "Epoch: 656, Loss: 0.9307, val_error: 0.4533\n",
            "Epoch: 657, Loss: 0.9301, val_error: 0.4621\n",
            "Epoch: 658, Loss: 0.8966, val_error: 0.4839\n",
            "Epoch: 659, Loss: 0.9261, val_error: 0.4819\n",
            "Epoch: 660, Loss: 0.8299, val_error: 0.4605\n",
            "Epoch: 661, Loss: 0.8804, val_error: 0.4787\n",
            "Epoch: 662, Loss: 0.9419, val_error: 0.4660\n",
            "Epoch: 663, Loss: 0.8544, val_error: 0.4799\n",
            "Epoch: 664, Loss: 0.8655, val_error: 0.4807\n",
            "Epoch: 665, Loss: 0.8128, val_error: 0.4779\n",
            "Epoch: 666, Loss: 0.8924, val_error: 0.5050\n",
            "Epoch: 667, Loss: 0.8726, val_error: 0.4589\n",
            "Epoch: 668, Loss: 0.8983, val_error: 0.4756\n",
            "Epoch: 669, Loss: 0.9057, val_error: 0.4791\n",
            "Epoch: 670, Loss: 0.9349, val_error: 0.4601\n",
            "Epoch: 671, Loss: 0.9498, val_error: 0.4613\n",
            "Epoch: 672, Loss: 0.8584, val_error: 0.4565\n",
            "Epoch: 673, Loss: 0.9224, val_error: 0.4350\n",
            "Epoch: 674, Loss: 0.9324, val_error: 0.4414\n",
            "Epoch: 675, Loss: 0.8733, val_error: 0.4485\n",
            "Epoch: 676, Loss: 0.9270, val_error: 0.4569\n",
            "Epoch: 677, Loss: 0.9177, val_error: 0.4776\n",
            "Epoch: 678, Loss: 0.9706, val_error: 0.4843\n",
            "Epoch: 679, Loss: 1.0274, val_error: 0.4859\n",
            "Epoch: 680, Loss: 0.9133, val_error: 0.4839\n",
            "Epoch: 681, Loss: 0.9127, val_error: 0.4883\n",
            "Epoch: 682, Loss: 0.8773, val_error: 0.5010\n",
            "Epoch: 683, Loss: 0.9686, val_error: 0.4899\n",
            "Epoch: 684, Loss: 0.8848, val_error: 0.4919\n",
            "Epoch: 685, Loss: 0.9850, val_error: 0.5030\n",
            "Epoch: 686, Loss: 0.8592, val_error: 0.4549\n",
            "Epoch: 687, Loss: 0.9343, val_error: 0.4422\n",
            "Epoch: 688, Loss: 0.8186, val_error: 0.4601\n",
            "Epoch: 689, Loss: 0.8718, val_error: 0.4466\n",
            "Epoch: 690, Loss: 0.8464, val_error: 0.4398\n",
            "Epoch: 691, Loss: 0.9618, val_error: 0.4736\n",
            "Epoch: 692, Loss: 0.8738, val_error: 0.4926\n",
            "Epoch: 693, Loss: 0.9144, val_error: 0.4875\n",
            "Epoch: 694, Loss: 0.8852, val_error: 0.4954\n",
            "Epoch: 695, Loss: 0.9140, val_error: 0.5113\n",
            "Epoch: 696, Loss: 0.9575, val_error: 0.5276\n",
            "Epoch: 697, Loss: 0.8693, val_error: 0.5137\n",
            "Epoch: 698, Loss: 0.9007, val_error: 0.4946\n",
            "Epoch: 699, Loss: 0.8824, val_error: 0.4903\n",
            "Epoch: 700, Loss: 0.9450, val_error: 0.4930\n",
            "Epoch: 701, Loss: 0.8599, val_error: 0.4883\n",
            "Epoch: 702, Loss: 0.8549, val_error: 0.4601\n",
            "Epoch: 703, Loss: 0.9300, val_error: 0.4664\n",
            "Epoch: 704, Loss: 0.9029, val_error: 0.4601\n",
            "Epoch: 705, Loss: 0.9034, val_error: 0.4406\n",
            "Epoch: 706, Loss: 0.9187, val_error: 0.4644\n",
            "Epoch: 707, Loss: 0.8912, val_error: 0.4362\n",
            "Epoch: 708, Loss: 0.8361, val_error: 0.4489\n",
            "Epoch: 709, Loss: 1.0026, val_error: 0.4636\n",
            "Epoch: 710, Loss: 0.8156, val_error: 0.4573\n",
            "Epoch: 711, Loss: 0.8974, val_error: 0.4831\n",
            "Epoch: 712, Loss: 0.9225, val_error: 0.4883\n",
            "Epoch: 713, Loss: 0.9061, val_error: 0.4791\n",
            "Epoch: 714, Loss: 0.9819, val_error: 0.4561\n",
            "Epoch: 715, Loss: 0.8092, val_error: 0.4736\n",
            "Epoch: 716, Loss: 0.8676, val_error: 0.4748\n",
            "Epoch: 717, Loss: 0.9186, val_error: 0.4779\n",
            "Epoch: 718, Loss: 0.9581, val_error: 0.4847\n",
            "Epoch: 719, Loss: 0.8077, val_error: 0.4875\n",
            "Epoch: 720, Loss: 0.9808, val_error: 0.5034\n",
            "Epoch: 721, Loss: 0.9174, val_error: 0.4640\n",
            "Epoch: 722, Loss: 0.9007, val_error: 0.4648\n",
            "Epoch: 723, Loss: 0.8950, val_error: 0.4470\n",
            "Epoch: 724, Loss: 0.9076, val_error: 0.4680\n",
            "Epoch: 725, Loss: 0.9038, val_error: 0.4629\n",
            "Epoch: 726, Loss: 0.8111, val_error: 0.4847\n",
            "Epoch: 727, Loss: 0.8223, val_error: 0.4879\n",
            "Epoch: 728, Loss: 0.9238, val_error: 0.4930\n",
            "Epoch: 729, Loss: 0.8903, val_error: 0.4875\n",
            "Epoch: 730, Loss: 0.9230, val_error: 0.4926\n",
            "Epoch: 731, Loss: 0.8478, val_error: 0.4839\n",
            "Epoch: 732, Loss: 0.9557, val_error: 0.4883\n",
            "Epoch: 733, Loss: 0.8281, val_error: 0.4974\n",
            "Epoch: 734, Loss: 0.8536, val_error: 0.4744\n",
            "Epoch: 735, Loss: 0.8637, val_error: 0.4835\n",
            "Epoch: 736, Loss: 0.8833, val_error: 0.4716\n",
            "Epoch: 737, Loss: 0.9188, val_error: 0.4561\n",
            "Epoch: 738, Loss: 0.8388, val_error: 0.4541\n",
            "Epoch: 739, Loss: 0.8686, val_error: 0.4601\n",
            "Epoch: 740, Loss: 0.9260, val_error: 0.4704\n",
            "Epoch: 741, Loss: 0.8741, val_error: 0.4791\n",
            "Epoch: 742, Loss: 0.9126, val_error: 0.4807\n",
            "Epoch: 743, Loss: 0.8855, val_error: 0.4875\n",
            "Epoch: 744, Loss: 1.0134, val_error: 0.5077\n",
            "Epoch: 745, Loss: 0.9108, val_error: 0.4926\n",
            "Epoch: 746, Loss: 0.9394, val_error: 0.4994\n",
            "Epoch: 747, Loss: 0.8495, val_error: 0.5105\n",
            "Epoch: 748, Loss: 0.8973, val_error: 0.5113\n",
            "Epoch: 749, Loss: 0.9038, val_error: 0.5022\n",
            "Epoch: 750, Loss: 0.9138, val_error: 0.5054\n",
            "Epoch: 751, Loss: 0.8217, val_error: 0.5046\n",
            "Epoch: 752, Loss: 0.9394, val_error: 0.5002\n",
            "Epoch: 753, Loss: 0.9474, val_error: 0.4982\n",
            "Epoch: 754, Loss: 0.9320, val_error: 0.4839\n",
            "Epoch: 755, Loss: 0.8353, val_error: 0.4891\n",
            "Epoch: 756, Loss: 0.8566, val_error: 0.5125\n",
            "Epoch: 757, Loss: 0.8573, val_error: 0.4994\n",
            "Epoch: 758, Loss: 0.8938, val_error: 0.5117\n",
            "Epoch: 759, Loss: 0.8676, val_error: 0.5181\n",
            "Epoch: 760, Loss: 0.9074, val_error: 0.5280\n",
            "Epoch: 761, Loss: 0.8417, val_error: 0.5173\n",
            "Epoch: 762, Loss: 0.8679, val_error: 0.5101\n",
            "Epoch: 763, Loss: 0.8774, val_error: 0.5312\n",
            "Epoch: 764, Loss: 0.8643, val_error: 0.5133\n",
            "Epoch: 765, Loss: 0.9044, val_error: 0.5050\n",
            "Epoch: 766, Loss: 0.8585, val_error: 0.5113\n",
            "Epoch: 767, Loss: 0.9010, val_error: 0.5074\n",
            "Epoch: 768, Loss: 0.8630, val_error: 0.5022\n",
            "Epoch: 769, Loss: 0.9628, val_error: 0.4942\n",
            "Epoch: 770, Loss: 0.8938, val_error: 0.5085\n",
            "Epoch: 771, Loss: 0.8594, val_error: 0.5145\n",
            "Epoch: 772, Loss: 0.9343, val_error: 0.5181\n",
            "Epoch: 773, Loss: 0.8152, val_error: 0.4930\n",
            "Epoch: 774, Loss: 0.9115, val_error: 0.5224\n",
            "Epoch: 775, Loss: 0.9443, val_error: 0.5070\n",
            "Epoch: 776, Loss: 0.8361, val_error: 0.5205\n",
            "Epoch: 777, Loss: 0.8947, val_error: 0.5022\n",
            "Epoch: 778, Loss: 0.8700, val_error: 0.5105\n",
            "Epoch: 779, Loss: 0.9498, val_error: 0.4970\n",
            "Epoch: 780, Loss: 0.9216, val_error: 0.4919\n",
            "Epoch: 781, Loss: 0.8208, val_error: 0.5157\n",
            "Epoch: 782, Loss: 0.9158, val_error: 0.5085\n",
            "Epoch: 783, Loss: 0.9466, val_error: 0.5157\n",
            "Epoch: 784, Loss: 0.8631, val_error: 0.4962\n",
            "Epoch: 785, Loss: 0.8695, val_error: 0.4946\n",
            "Epoch: 786, Loss: 0.8932, val_error: 0.5062\n",
            "Epoch: 787, Loss: 0.8191, val_error: 0.4974\n",
            "Epoch: 788, Loss: 0.9206, val_error: 0.5070\n",
            "Epoch: 789, Loss: 0.8835, val_error: 0.5157\n",
            "Epoch: 790, Loss: 0.8380, val_error: 0.5125\n",
            "Epoch: 791, Loss: 0.8855, val_error: 0.4930\n",
            "Epoch: 792, Loss: 1.0255, val_error: 0.4934\n",
            "Epoch: 793, Loss: 0.8452, val_error: 0.4938\n",
            "Epoch: 794, Loss: 0.9294, val_error: 0.4911\n",
            "Epoch: 795, Loss: 0.8086, val_error: 0.5165\n",
            "Epoch: 796, Loss: 0.8948, val_error: 0.5272\n",
            "Epoch: 797, Loss: 0.8676, val_error: 0.5272\n",
            "Epoch: 798, Loss: 0.8856, val_error: 0.5236\n",
            "Epoch: 799, Loss: 0.8360, val_error: 0.5153\n",
            "Epoch: 800, Loss: 0.8953, val_error: 0.5221\n",
            "Epoch: 801, Loss: 0.9816, val_error: 0.5308\n",
            "Epoch: 802, Loss: 0.9096, val_error: 0.5260\n",
            "Epoch: 803, Loss: 0.8729, val_error: 0.5097\n",
            "Epoch: 804, Loss: 0.8639, val_error: 0.5272\n",
            "Epoch: 805, Loss: 0.9551, val_error: 0.5093\n",
            "Epoch: 806, Loss: 0.8284, val_error: 0.5419\n",
            "Epoch: 807, Loss: 0.9647, val_error: 0.5197\n",
            "Epoch: 808, Loss: 0.8804, val_error: 0.5224\n",
            "Epoch: 809, Loss: 0.8059, val_error: 0.5058\n",
            "Epoch: 810, Loss: 0.9709, val_error: 0.4915\n",
            "Epoch: 811, Loss: 0.8628, val_error: 0.4736\n",
            "Epoch: 812, Loss: 0.8649, val_error: 0.4803\n",
            "Epoch: 813, Loss: 0.8257, val_error: 0.4589\n",
            "Epoch: 814, Loss: 0.8660, val_error: 0.4926\n",
            "Epoch: 815, Loss: 0.9104, val_error: 0.4950\n",
            "Epoch: 816, Loss: 0.8630, val_error: 0.4887\n",
            "Epoch: 817, Loss: 0.9836, val_error: 0.4831\n",
            "Epoch: 818, Loss: 0.9493, val_error: 0.5046\n",
            "Epoch: 819, Loss: 0.9219, val_error: 0.4923\n",
            "Epoch: 820, Loss: 0.8779, val_error: 0.5062\n",
            "Epoch: 821, Loss: 0.9551, val_error: 0.5272\n",
            "Epoch: 822, Loss: 0.9053, val_error: 0.5415\n",
            "Epoch: 823, Loss: 0.8661, val_error: 0.5542\n",
            "Epoch: 824, Loss: 0.9466, val_error: 0.5312\n",
            "Epoch: 825, Loss: 0.8358, val_error: 0.4994\n",
            "Epoch: 826, Loss: 0.8548, val_error: 0.5077\n",
            "Epoch: 827, Loss: 0.9308, val_error: 0.4895\n",
            "Epoch: 828, Loss: 0.8870, val_error: 0.4819\n",
            "Epoch: 829, Loss: 0.9322, val_error: 0.4716\n",
            "Epoch: 830, Loss: 1.0095, val_error: 0.4676\n",
            "Epoch: 831, Loss: 0.8384, val_error: 0.4668\n",
            "Epoch: 832, Loss: 0.8221, val_error: 0.4811\n",
            "Epoch: 833, Loss: 0.9812, val_error: 0.4934\n",
            "Epoch: 834, Loss: 0.8502, val_error: 0.4851\n",
            "Epoch: 835, Loss: 0.8644, val_error: 0.5006\n",
            "Epoch: 836, Loss: 0.8934, val_error: 0.5014\n",
            "Epoch: 837, Loss: 0.9165, val_error: 0.5081\n",
            "Epoch: 838, Loss: 0.8780, val_error: 0.5117\n",
            "Epoch: 839, Loss: 1.0163, val_error: 0.4895\n",
            "Epoch: 840, Loss: 0.8876, val_error: 0.5336\n",
            "Epoch: 841, Loss: 0.8280, val_error: 0.5264\n",
            "Epoch: 842, Loss: 0.9348, val_error: 0.5336\n",
            "Epoch: 843, Loss: 0.9373, val_error: 0.5284\n",
            "Epoch: 844, Loss: 0.9376, val_error: 0.4934\n",
            "Epoch: 845, Loss: 0.9002, val_error: 0.5042\n",
            "Epoch: 846, Loss: 0.9160, val_error: 0.5522\n",
            "Epoch: 847, Loss: 0.8510, val_error: 0.5320\n",
            "Epoch: 848, Loss: 0.9693, val_error: 0.5189\n",
            "Epoch: 849, Loss: 0.8864, val_error: 0.5360\n",
            "Epoch: 850, Loss: 0.8303, val_error: 0.5320\n",
            "Epoch: 851, Loss: 0.8489, val_error: 0.5395\n",
            "Epoch: 852, Loss: 0.9707, val_error: 0.5431\n",
            "Epoch: 853, Loss: 0.9393, val_error: 0.5383\n",
            "Epoch: 854, Loss: 0.8298, val_error: 0.5407\n",
            "Epoch: 855, Loss: 0.9293, val_error: 0.5320\n",
            "Epoch: 856, Loss: 0.8991, val_error: 0.5165\n",
            "Epoch: 857, Loss: 0.9078, val_error: 0.5137\n",
            "Epoch: 858, Loss: 0.9500, val_error: 0.5145\n",
            "Epoch: 859, Loss: 0.8554, val_error: 0.4994\n",
            "Epoch: 860, Loss: 0.8585, val_error: 0.5042\n",
            "Epoch: 861, Loss: 0.8478, val_error: 0.5002\n",
            "Epoch: 862, Loss: 0.8856, val_error: 0.5244\n",
            "Epoch: 863, Loss: 0.8337, val_error: 0.5097\n",
            "Epoch: 864, Loss: 0.9349, val_error: 0.5491\n",
            "Epoch: 865, Loss: 0.8543, val_error: 0.5534\n",
            "Epoch: 866, Loss: 0.9375, val_error: 0.5634\n",
            "Epoch: 867, Loss: 1.0044, val_error: 0.5578\n",
            "Epoch: 868, Loss: 0.9195, val_error: 0.5697\n",
            "Epoch: 869, Loss: 0.8118, val_error: 0.5518\n",
            "Epoch: 870, Loss: 0.8481, val_error: 0.5352\n",
            "Epoch: 871, Loss: 0.8661, val_error: 0.5276\n",
            "Epoch: 872, Loss: 0.8880, val_error: 0.5280\n",
            "Epoch: 873, Loss: 0.8777, val_error: 0.5089\n",
            "Epoch: 874, Loss: 1.0080, val_error: 0.4934\n",
            "Epoch: 875, Loss: 0.8710, val_error: 0.5077\n",
            "Epoch: 876, Loss: 0.9300, val_error: 0.5129\n",
            "Epoch: 877, Loss: 0.8859, val_error: 0.5268\n",
            "Epoch: 878, Loss: 0.8592, val_error: 0.5383\n",
            "Epoch: 879, Loss: 0.9881, val_error: 0.5693\n",
            "Epoch: 880, Loss: 0.8027, val_error: 0.5526\n",
            "Epoch: 881, Loss: 0.8260, val_error: 0.5773\n",
            "Epoch: 882, Loss: 0.8118, val_error: 0.5662\n",
            "Epoch: 883, Loss: 0.8774, val_error: 0.5515\n",
            "Epoch: 884, Loss: 0.9286, val_error: 0.5646\n",
            "Epoch: 885, Loss: 0.8626, val_error: 0.5375\n",
            "Epoch: 886, Loss: 0.8161, val_error: 0.5340\n",
            "Epoch: 887, Loss: 0.9089, val_error: 0.5181\n",
            "Epoch: 888, Loss: 0.8746, val_error: 0.5177\n",
            "Epoch: 889, Loss: 0.9103, val_error: 0.5081\n",
            "Epoch: 890, Loss: 0.8673, val_error: 0.5121\n",
            "Epoch: 891, Loss: 0.8098, val_error: 0.5022\n",
            "Epoch: 892, Loss: 0.9246, val_error: 0.5197\n",
            "Epoch: 893, Loss: 0.8292, val_error: 0.5284\n",
            "Epoch: 894, Loss: 0.8678, val_error: 0.5383\n",
            "Epoch: 895, Loss: 0.8954, val_error: 0.5177\n",
            "Epoch: 896, Loss: 0.8649, val_error: 0.5451\n",
            "Epoch: 897, Loss: 0.8314, val_error: 0.5368\n",
            "Epoch: 898, Loss: 0.9025, val_error: 0.5431\n",
            "Epoch: 899, Loss: 0.8363, val_error: 0.5276\n",
            "Epoch: 900, Loss: 0.9038, val_error: 0.5252\n",
            "Epoch: 901, Loss: 0.8839, val_error: 0.5340\n",
            "Epoch: 902, Loss: 0.8154, val_error: 0.5495\n",
            "Epoch: 903, Loss: 0.8506, val_error: 0.5455\n",
            "Epoch: 904, Loss: 0.7850, val_error: 0.5475\n",
            "Epoch: 905, Loss: 0.8009, val_error: 0.5248\n",
            "Epoch: 906, Loss: 0.8881, val_error: 0.5157\n",
            "Epoch: 907, Loss: 0.8930, val_error: 0.4954\n",
            "Epoch: 908, Loss: 0.8334, val_error: 0.4831\n",
            "Epoch: 909, Loss: 0.8745, val_error: 0.4859\n",
            "Epoch: 910, Loss: 1.0327, val_error: 0.4899\n",
            "Epoch: 911, Loss: 0.8990, val_error: 0.4640\n",
            "Epoch: 912, Loss: 0.8413, val_error: 0.4934\n",
            "Epoch: 913, Loss: 0.8459, val_error: 0.4915\n",
            "Epoch: 914, Loss: 0.8585, val_error: 0.4926\n",
            "Epoch: 915, Loss: 0.8298, val_error: 0.5248\n",
            "Epoch: 916, Loss: 0.9038, val_error: 0.5395\n",
            "Epoch: 917, Loss: 0.8899, val_error: 0.5360\n",
            "Epoch: 918, Loss: 0.8928, val_error: 0.5340\n",
            "Epoch: 919, Loss: 0.9266, val_error: 0.5503\n",
            "Epoch: 920, Loss: 0.9059, val_error: 0.5352\n",
            "Epoch: 921, Loss: 0.9257, val_error: 0.5530\n",
            "Epoch: 922, Loss: 0.8618, val_error: 0.5320\n",
            "Epoch: 923, Loss: 0.7727, val_error: 0.5177\n",
            "Epoch: 924, Loss: 0.8299, val_error: 0.4879\n",
            "Epoch: 925, Loss: 0.9404, val_error: 0.4907\n",
            "Epoch: 926, Loss: 0.9301, val_error: 0.4772\n",
            "Epoch: 927, Loss: 0.9766, val_error: 0.4990\n",
            "Epoch: 928, Loss: 0.9410, val_error: 0.4756\n",
            "Epoch: 929, Loss: 0.9130, val_error: 0.4934\n",
            "Epoch: 930, Loss: 0.8528, val_error: 0.4883\n",
            "Epoch: 931, Loss: 0.8432, val_error: 0.4859\n",
            "Epoch: 932, Loss: 0.8920, val_error: 0.5077\n",
            "Epoch: 933, Loss: 0.9854, val_error: 0.5030\n",
            "Epoch: 934, Loss: 0.8739, val_error: 0.5133\n",
            "Epoch: 935, Loss: 0.8095, val_error: 0.5054\n",
            "Epoch: 936, Loss: 0.8329, val_error: 0.4926\n",
            "Epoch: 937, Loss: 0.9828, val_error: 0.5201\n",
            "Epoch: 938, Loss: 0.8870, val_error: 0.5268\n",
            "Epoch: 939, Loss: 0.8914, val_error: 0.5153\n",
            "Epoch: 940, Loss: 0.8797, val_error: 0.5280\n",
            "Epoch: 941, Loss: 0.8095, val_error: 0.5280\n",
            "Epoch: 942, Loss: 0.9103, val_error: 0.5236\n",
            "Epoch: 943, Loss: 0.8287, val_error: 0.5228\n",
            "Epoch: 944, Loss: 0.8359, val_error: 0.5129\n",
            "Epoch: 945, Loss: 0.8252, val_error: 0.5316\n",
            "Epoch: 946, Loss: 0.8181, val_error: 0.5236\n",
            "Epoch: 947, Loss: 0.8317, val_error: 0.5371\n",
            "Epoch: 948, Loss: 0.8603, val_error: 0.5407\n",
            "Epoch: 949, Loss: 0.8837, val_error: 0.5356\n",
            "Epoch: 950, Loss: 0.8367, val_error: 0.5105\n",
            "Epoch: 951, Loss: 0.9382, val_error: 0.5165\n",
            "Epoch: 952, Loss: 0.8500, val_error: 0.4879\n",
            "Epoch: 953, Loss: 0.8249, val_error: 0.5074\n",
            "Epoch: 954, Loss: 0.8470, val_error: 0.5089\n",
            "Epoch: 955, Loss: 0.8993, val_error: 0.5145\n",
            "Epoch: 956, Loss: 0.8427, val_error: 0.5105\n",
            "Epoch: 957, Loss: 0.8942, val_error: 0.5316\n",
            "Epoch: 958, Loss: 0.8870, val_error: 0.5499\n",
            "Epoch: 959, Loss: 0.8568, val_error: 0.5431\n",
            "Epoch: 960, Loss: 0.9514, val_error: 0.5558\n",
            "Epoch: 961, Loss: 0.9011, val_error: 0.5518\n",
            "Epoch: 962, Loss: 0.8588, val_error: 0.5181\n",
            "Epoch: 963, Loss: 0.7886, val_error: 0.5213\n",
            "Epoch: 964, Loss: 0.8784, val_error: 0.5014\n",
            "Epoch: 965, Loss: 1.0436, val_error: 0.4958\n",
            "Epoch: 966, Loss: 0.8669, val_error: 0.5101\n",
            "Epoch: 967, Loss: 0.8234, val_error: 0.4986\n",
            "Epoch: 968, Loss: 0.8594, val_error: 0.5042\n",
            "Epoch: 969, Loss: 0.8546, val_error: 0.5046\n",
            "Epoch: 970, Loss: 0.8772, val_error: 0.4950\n",
            "Epoch: 971, Loss: 0.9044, val_error: 0.4942\n",
            "Epoch: 972, Loss: 0.9267, val_error: 0.4930\n",
            "Epoch: 973, Loss: 0.8513, val_error: 0.5085\n",
            "Epoch: 974, Loss: 0.8654, val_error: 0.5085\n",
            "Epoch: 975, Loss: 0.8778, val_error: 0.5316\n",
            "Epoch: 976, Loss: 0.8676, val_error: 0.5248\n",
            "Epoch: 977, Loss: 0.8502, val_error: 0.5284\n",
            "Epoch: 978, Loss: 0.8220, val_error: 0.5268\n",
            "Epoch: 979, Loss: 0.8752, val_error: 0.5368\n",
            "Epoch: 980, Loss: 0.8285, val_error: 0.5205\n",
            "Epoch: 981, Loss: 0.8219, val_error: 0.5288\n",
            "Epoch: 982, Loss: 0.8098, val_error: 0.5224\n",
            "Epoch: 983, Loss: 0.8215, val_error: 0.5371\n",
            "Epoch: 984, Loss: 0.9254, val_error: 0.5280\n",
            "Epoch: 985, Loss: 0.8219, val_error: 0.5383\n",
            "Epoch: 986, Loss: 0.8287, val_error: 0.5288\n",
            "Epoch: 987, Loss: 0.8408, val_error: 0.5352\n",
            "Epoch: 988, Loss: 0.8508, val_error: 0.5328\n",
            "Epoch: 989, Loss: 0.8863, val_error: 0.5324\n",
            "Epoch: 990, Loss: 0.8459, val_error: 0.5161\n",
            "Epoch: 991, Loss: 0.8466, val_error: 0.5296\n",
            "Epoch: 992, Loss: 0.8720, val_error: 0.5288\n",
            "Epoch: 993, Loss: 0.8077, val_error: 0.5411\n",
            "Epoch: 994, Loss: 1.0015, val_error: 0.5081\n",
            "Epoch: 995, Loss: 0.8239, val_error: 0.4923\n",
            "Epoch: 996, Loss: 0.8532, val_error: 0.5010\n",
            "Epoch: 997, Loss: 0.8028, val_error: 0.5105\n",
            "Epoch: 998, Loss: 0.8242, val_error: 0.5054\n",
            "Epoch: 999, Loss: 0.9109, val_error: 0.4847\n",
            "Epoch: 1000, Loss: 0.8753, val_error: 0.5177\n"
          ]
        }
      ],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "from IPython.display import Javascript  # Restrict height of output cell.\n",
        "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
        "\n",
        "model = GCN(hidden_channels=16)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
        "#criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "def train():\n",
        "      model.train()\n",
        "      optimizer.zero_grad()  # Clear gradients.\n",
        "      out = model(graph.x, graph.edge_index)  # Perform a single forward pass.\n",
        "      loss = criterion(out[graph.train_mask], graph.y[graph.train_mask])  # Compute the loss solely based on the training nodes.\n",
        "      loss.backward()  # Derive gradients.\n",
        "      optimizer.step()  # Update parameters based on gradients.\n",
        "\n",
        "      # --- Validation error ---\n",
        "      model.eval()  # switch to eval mode (disables dropout, etc.)\n",
        "      with torch.no_grad():\n",
        "            pred = out.argmax(dim=1)\n",
        "            val_pred = pred[graph.val_mask]\n",
        "            val_true = graph.y[graph.val_mask]\n",
        "            val_acc = (val_pred == val_true).float().mean().item()\n",
        "    \n",
        "      return loss.item(), val_acc\n",
        "\n",
        "def test():\n",
        "      model.eval()\n",
        "      out = model(graph.x, graph.edge_index)\n",
        "      pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
        "      test_correct = pred[graph.test_mask] == graph.y[graph.test_mask]  # Check against ground-truth labels.\n",
        "      test_acc = int(test_correct.sum()) / int(graph.test_mask.sum())  # Derive ratio of correct predictions.\n",
        "      return test_acc\n",
        "\n",
        "\n",
        "for epoch in range(1, 1001):\n",
        "    loss, val_acc = train()\n",
        "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, val_error: {val_acc:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Report:\n",
            " {'0': {'precision': 0.013888888888888888, 'recall': 0.058823529411764705, 'f1-score': 0.02247191011235955, 'support': 17.0}, '1': {'precision': 0.9584799437016186, 'recall': 0.5632754342431762, 'f1-score': 0.709559781193019, 'support': 2418.0}, '2': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 24.0}, '3': {'precision': 0.006756756756756757, 'recall': 0.0625, 'f1-score': 0.012195121951219513, 'support': 16.0}, '4': {'precision': 0.003778337531486146, 'recall': 0.1875, 'f1-score': 0.007407407407407408, 'support': 16.0}, '5': {'precision': 0.02631578947368421, 'recall': 0.038461538461538464, 'f1-score': 0.03125, 'support': 26.0}, 'accuracy': 0.5435041716328963, 'macro avg': {'precision': 0.1682032860587391, 'recall': 0.15176008368607988, 'f1-score': 0.13048070344400092, 'support': 2517.0}, 'weighted avg': {'precision': 0.921213105687943, 'recall': 0.5435041716328963, 'f1-score': 0.6822501842933526, 'support': 2517.0}}\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Assume:\n",
        "# - y_true: ground truth labels of test nodes\n",
        "# - y_pred: predicted labels of test nodes\n",
        "\n",
        "# Example:\n",
        "y_pred = model(graph.x, graph.edge_index).argmax(dim=1)\n",
        "y_true = graph.y\n",
        "\n",
        "# Use the test mask to slice out test nodes\n",
        "y_pred_test = y_pred[graph.test_mask]\n",
        "y_true_test = y_true[graph.test_mask]\n",
        "\n",
        "# Classification report (includes precision, recall, F1 per class)\n",
        "report = classification_report(y_true_test.cpu(), y_pred_test.cpu(), digits=3, output_dict=True)\n",
        "print(\"Classification Report:\\n\", report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6822501842933526"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#f1_weighted_GCN_GNG = report[\"weighted avg\"][\"f1-score\"]\n",
        "#f1_weighted_GCN_GNG\n",
        "f1_weighted_GCN_SMP_R = report[\"weighted avg\"][\"f1-score\"]\n",
        "f1_weighted_GCN_SMP_R"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GAT(\n",
            "  (conv1): GATConv(6, 16, heads=8)\n",
            "  (conv2): GATConv(128, 6, heads=1)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "from torch_geometric.nn import GATConv\n",
        "\n",
        "# Assuming num_features and num_classes are defined globally\n",
        "# e.g., num_features = dataset.num_features, num_classes = dataset.num_classes\n",
        "\n",
        "class GAT(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super().__init__()\n",
        "        torch.manual_seed(1234567)\n",
        "        self.conv1 = GATConv(num_features, hidden_channels, heads=8, dropout=0.0)\n",
        "        # Output dim of conv1 is hidden_channels * heads, so we set in_channels accordingly\n",
        "        self.conv2 = GATConv(hidden_channels * 8, num_classes, heads=1, concat=False, dropout=0.0)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.elu(x)\n",
        "        x = F.dropout(x, p=0.5, training=self.training)  # Dropout only active during training\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "model = GAT(hidden_channels=16)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/javascript": "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 001, Loss: 1.7874, val_error: 0.1442\n",
            "Epoch: 002, Loss: 1.7570, val_error: 0.4998\n",
            "Epoch: 003, Loss: 1.7265, val_error: 0.5419\n",
            "Epoch: 004, Loss: 1.6803, val_error: 0.5538\n",
            "Epoch: 005, Loss: 1.6605, val_error: 0.5157\n",
            "Epoch: 006, Loss: 1.6137, val_error: 0.4648\n",
            "Epoch: 007, Loss: 1.6030, val_error: 0.4025\n",
            "Epoch: 008, Loss: 1.5797, val_error: 0.3603\n",
            "Epoch: 009, Loss: 1.5633, val_error: 0.3031\n",
            "Epoch: 010, Loss: 1.5104, val_error: 0.2634\n",
            "Epoch: 011, Loss: 1.5064, val_error: 0.2169\n",
            "Epoch: 012, Loss: 1.4755, val_error: 0.1824\n",
            "Epoch: 013, Loss: 1.4715, val_error: 0.1474\n",
            "Epoch: 014, Loss: 1.4086, val_error: 0.1232\n",
            "Epoch: 015, Loss: 1.3895, val_error: 0.1188\n",
            "Epoch: 016, Loss: 1.3811, val_error: 0.1152\n",
            "Epoch: 017, Loss: 1.3708, val_error: 0.1116\n",
            "Epoch: 018, Loss: 1.3325, val_error: 0.1045\n",
            "Epoch: 019, Loss: 1.3117, val_error: 0.1156\n",
            "Epoch: 020, Loss: 1.2997, val_error: 0.1065\n",
            "Epoch: 021, Loss: 1.2633, val_error: 0.0989\n",
            "Epoch: 022, Loss: 1.2514, val_error: 0.1045\n",
            "Epoch: 023, Loss: 1.2328, val_error: 0.0993\n",
            "Epoch: 024, Loss: 1.2260, val_error: 0.1001\n",
            "Epoch: 025, Loss: 1.1846, val_error: 0.0822\n",
            "Epoch: 026, Loss: 1.1932, val_error: 0.1097\n",
            "Epoch: 027, Loss: 1.1548, val_error: 0.1279\n",
            "Epoch: 028, Loss: 1.1321, val_error: 0.1605\n",
            "Epoch: 029, Loss: 1.1460, val_error: 0.2050\n",
            "Epoch: 030, Loss: 1.0932, val_error: 0.2145\n",
            "Epoch: 031, Loss: 1.0981, val_error: 0.2610\n",
            "Epoch: 032, Loss: 1.0815, val_error: 0.2296\n",
            "Epoch: 033, Loss: 1.0533, val_error: 0.1983\n",
            "Epoch: 034, Loss: 1.0469, val_error: 0.1534\n",
            "Epoch: 035, Loss: 1.0227, val_error: 0.1327\n",
            "Epoch: 036, Loss: 1.0145, val_error: 0.1152\n",
            "Epoch: 037, Loss: 1.0207, val_error: 0.1251\n",
            "Epoch: 038, Loss: 0.9937, val_error: 0.1160\n",
            "Epoch: 039, Loss: 0.9632, val_error: 0.1128\n",
            "Epoch: 040, Loss: 0.9482, val_error: 0.1291\n",
            "Epoch: 041, Loss: 0.9370, val_error: 0.1343\n",
            "Epoch: 042, Loss: 0.9282, val_error: 0.1617\n",
            "Epoch: 043, Loss: 1.0484, val_error: 0.1601\n",
            "Epoch: 044, Loss: 0.8843, val_error: 0.1371\n",
            "Epoch: 045, Loss: 0.8937, val_error: 0.1299\n",
            "Epoch: 046, Loss: 0.9076, val_error: 0.1192\n",
            "Epoch: 047, Loss: 0.8602, val_error: 0.1263\n",
            "Epoch: 048, Loss: 0.8492, val_error: 0.1498\n",
            "Epoch: 049, Loss: 0.9137, val_error: 0.1681\n",
            "Epoch: 050, Loss: 0.8626, val_error: 0.1788\n",
            "Epoch: 051, Loss: 0.8613, val_error: 0.1756\n",
            "Epoch: 052, Loss: 0.8207, val_error: 0.1816\n",
            "Epoch: 053, Loss: 0.7954, val_error: 0.1589\n",
            "Epoch: 054, Loss: 0.8200, val_error: 0.1208\n",
            "Epoch: 055, Loss: 0.8164, val_error: 0.1148\n",
            "Epoch: 056, Loss: 0.7438, val_error: 0.1104\n",
            "Epoch: 057, Loss: 0.7926, val_error: 0.1220\n",
            "Epoch: 058, Loss: 0.7295, val_error: 0.1395\n",
            "Epoch: 059, Loss: 0.7304, val_error: 0.1657\n",
            "Epoch: 060, Loss: 0.7075, val_error: 0.2280\n",
            "Epoch: 061, Loss: 0.7364, val_error: 0.2682\n",
            "Epoch: 062, Loss: 0.6840, val_error: 0.2753\n",
            "Epoch: 063, Loss: 0.7284, val_error: 0.3123\n",
            "Epoch: 064, Loss: 0.7041, val_error: 0.2988\n",
            "Epoch: 065, Loss: 0.6279, val_error: 0.2781\n",
            "Epoch: 066, Loss: 0.6476, val_error: 0.2586\n",
            "Epoch: 067, Loss: 0.6508, val_error: 0.2126\n",
            "Epoch: 068, Loss: 0.6326, val_error: 0.2157\n",
            "Epoch: 069, Loss: 0.5906, val_error: 0.2122\n",
            "Epoch: 070, Loss: 0.6198, val_error: 0.2356\n",
            "Epoch: 071, Loss: 0.5657, val_error: 0.2868\n",
            "Epoch: 072, Loss: 0.5429, val_error: 0.3218\n",
            "Epoch: 073, Loss: 0.5205, val_error: 0.3492\n",
            "Epoch: 074, Loss: 0.5460, val_error: 0.3953\n",
            "Epoch: 075, Loss: 0.5107, val_error: 0.3901\n",
            "Epoch: 076, Loss: 0.4786, val_error: 0.4207\n",
            "Epoch: 077, Loss: 0.4763, val_error: 0.4172\n",
            "Epoch: 078, Loss: 0.4629, val_error: 0.4148\n",
            "Epoch: 079, Loss: 0.3859, val_error: 0.3997\n",
            "Epoch: 080, Loss: 0.3847, val_error: 0.3842\n",
            "Epoch: 081, Loss: 0.3611, val_error: 0.4084\n",
            "Epoch: 082, Loss: 0.4180, val_error: 0.4164\n",
            "Epoch: 083, Loss: 0.3860, val_error: 0.4307\n",
            "Epoch: 084, Loss: 0.3881, val_error: 0.4275\n",
            "Epoch: 085, Loss: 0.4185, val_error: 0.4386\n",
            "Epoch: 086, Loss: 0.3434, val_error: 0.4446\n",
            "Epoch: 087, Loss: 0.3230, val_error: 0.5077\n",
            "Epoch: 088, Loss: 0.3313, val_error: 0.5431\n",
            "Epoch: 089, Loss: 0.3016, val_error: 0.5328\n",
            "Epoch: 090, Loss: 0.2834, val_error: 0.5665\n",
            "Epoch: 091, Loss: 0.3200, val_error: 0.5669\n",
            "Epoch: 092, Loss: 0.3258, val_error: 0.5491\n",
            "Epoch: 093, Loss: 0.4075, val_error: 0.5554\n",
            "Epoch: 094, Loss: 0.2932, val_error: 0.5356\n",
            "Epoch: 095, Loss: 0.2668, val_error: 0.4919\n",
            "Epoch: 096, Loss: 0.2859, val_error: 0.5209\n",
            "Epoch: 097, Loss: 0.3125, val_error: 0.4720\n",
            "Epoch: 098, Loss: 0.3206, val_error: 0.5193\n",
            "Epoch: 099, Loss: 0.2674, val_error: 0.5109\n",
            "Epoch: 100, Loss: 0.3116, val_error: 0.5654\n",
            "Epoch: 101, Loss: 0.3122, val_error: 0.5828\n",
            "Epoch: 102, Loss: 0.3051, val_error: 0.6544\n",
            "Epoch: 103, Loss: 0.2423, val_error: 0.6718\n",
            "Epoch: 104, Loss: 0.1945, val_error: 0.6683\n",
            "Epoch: 105, Loss: 0.3291, val_error: 0.6607\n",
            "Epoch: 106, Loss: 0.2605, val_error: 0.6595\n",
            "Epoch: 107, Loss: 0.2798, val_error: 0.7032\n",
            "Epoch: 108, Loss: 0.2702, val_error: 0.6806\n",
            "Epoch: 109, Loss: 0.2135, val_error: 0.6365\n",
            "Epoch: 110, Loss: 0.1925, val_error: 0.6675\n",
            "Epoch: 111, Loss: 0.2045, val_error: 0.6373\n",
            "Epoch: 112, Loss: 0.3496, val_error: 0.6393\n",
            "Epoch: 113, Loss: 0.2051, val_error: 0.6516\n",
            "Epoch: 114, Loss: 0.2084, val_error: 0.6035\n",
            "Epoch: 115, Loss: 0.2307, val_error: 0.6174\n",
            "Epoch: 116, Loss: 0.2120, val_error: 0.6643\n",
            "Epoch: 117, Loss: 0.2272, val_error: 0.6869\n",
            "Epoch: 118, Loss: 0.2307, val_error: 0.6706\n",
            "Epoch: 119, Loss: 0.3813, val_error: 0.6849\n",
            "Epoch: 120, Loss: 0.1925, val_error: 0.7020\n",
            "Epoch: 121, Loss: 0.4065, val_error: 0.7580\n",
            "Epoch: 122, Loss: 0.2130, val_error: 0.7183\n",
            "Epoch: 123, Loss: 0.2289, val_error: 0.7314\n",
            "Epoch: 124, Loss: 0.2411, val_error: 0.7088\n",
            "Epoch: 125, Loss: 0.2483, val_error: 0.6905\n",
            "Epoch: 126, Loss: 0.2518, val_error: 0.6611\n",
            "Epoch: 127, Loss: 0.2066, val_error: 0.6253\n",
            "Epoch: 128, Loss: 0.2395, val_error: 0.5586\n",
            "Epoch: 129, Loss: 0.1620, val_error: 0.5904\n",
            "Epoch: 130, Loss: 0.1946, val_error: 0.5598\n",
            "Epoch: 131, Loss: 0.2262, val_error: 0.5324\n",
            "Epoch: 132, Loss: 0.1670, val_error: 0.5642\n",
            "Epoch: 133, Loss: 0.2078, val_error: 0.5379\n",
            "Epoch: 134, Loss: 0.2017, val_error: 0.5709\n",
            "Epoch: 135, Loss: 0.2209, val_error: 0.6345\n",
            "Epoch: 136, Loss: 0.1741, val_error: 0.6130\n",
            "Epoch: 137, Loss: 0.2277, val_error: 0.6055\n",
            "Epoch: 138, Loss: 0.2253, val_error: 0.6099\n",
            "Epoch: 139, Loss: 0.1943, val_error: 0.6257\n",
            "Epoch: 140, Loss: 0.3134, val_error: 0.6838\n",
            "Epoch: 141, Loss: 0.2651, val_error: 0.6289\n",
            "Epoch: 142, Loss: 0.2677, val_error: 0.6297\n",
            "Epoch: 143, Loss: 0.1574, val_error: 0.6250\n",
            "Epoch: 144, Loss: 0.2247, val_error: 0.6770\n",
            "Epoch: 145, Loss: 0.1436, val_error: 0.6706\n",
            "Epoch: 146, Loss: 0.2109, val_error: 0.6834\n",
            "Epoch: 147, Loss: 0.1847, val_error: 0.6659\n",
            "Epoch: 148, Loss: 0.2550, val_error: 0.6977\n",
            "Epoch: 149, Loss: 0.1647, val_error: 0.6929\n",
            "Epoch: 150, Loss: 0.2298, val_error: 0.6551\n",
            "Epoch: 151, Loss: 0.2154, val_error: 0.6599\n",
            "Epoch: 152, Loss: 0.1724, val_error: 0.6722\n",
            "Epoch: 153, Loss: 0.1734, val_error: 0.6603\n",
            "Epoch: 154, Loss: 0.1913, val_error: 0.6782\n",
            "Epoch: 155, Loss: 0.2147, val_error: 0.6933\n",
            "Epoch: 156, Loss: 0.2023, val_error: 0.6464\n",
            "Epoch: 157, Loss: 0.1539, val_error: 0.6349\n",
            "Epoch: 158, Loss: 0.1722, val_error: 0.6257\n",
            "Epoch: 159, Loss: 0.1717, val_error: 0.6488\n",
            "Epoch: 160, Loss: 0.1808, val_error: 0.6520\n",
            "Epoch: 161, Loss: 0.1987, val_error: 0.6651\n",
            "Epoch: 162, Loss: 0.1835, val_error: 0.6591\n",
            "Epoch: 163, Loss: 0.1497, val_error: 0.6762\n",
            "Epoch: 164, Loss: 0.1482, val_error: 0.7068\n",
            "Epoch: 165, Loss: 0.1627, val_error: 0.6738\n",
            "Epoch: 166, Loss: 0.1595, val_error: 0.6694\n",
            "Epoch: 167, Loss: 0.2014, val_error: 0.6722\n",
            "Epoch: 168, Loss: 0.1669, val_error: 0.6861\n",
            "Epoch: 169, Loss: 0.1428, val_error: 0.6226\n",
            "Epoch: 170, Loss: 0.1517, val_error: 0.6516\n",
            "Epoch: 171, Loss: 0.1853, val_error: 0.5991\n",
            "Epoch: 172, Loss: 0.1504, val_error: 0.6242\n",
            "Epoch: 173, Loss: 0.1685, val_error: 0.6186\n",
            "Epoch: 174, Loss: 0.1280, val_error: 0.6369\n",
            "Epoch: 175, Loss: 0.1426, val_error: 0.6730\n",
            "Epoch: 176, Loss: 0.1628, val_error: 0.6456\n",
            "Epoch: 177, Loss: 0.1683, val_error: 0.7064\n",
            "Epoch: 178, Loss: 0.1486, val_error: 0.7191\n",
            "Epoch: 179, Loss: 0.1568, val_error: 0.6798\n",
            "Epoch: 180, Loss: 0.1607, val_error: 0.7215\n",
            "Epoch: 181, Loss: 0.1507, val_error: 0.7243\n",
            "Epoch: 182, Loss: 0.1334, val_error: 0.6985\n",
            "Epoch: 183, Loss: 0.1479, val_error: 0.6754\n",
            "Epoch: 184, Loss: 0.2205, val_error: 0.6929\n",
            "Epoch: 185, Loss: 0.1702, val_error: 0.6877\n",
            "Epoch: 186, Loss: 0.1552, val_error: 0.6301\n",
            "Epoch: 187, Loss: 0.1679, val_error: 0.6528\n",
            "Epoch: 188, Loss: 0.1701, val_error: 0.6130\n",
            "Epoch: 189, Loss: 0.1579, val_error: 0.5904\n",
            "Epoch: 190, Loss: 0.1566, val_error: 0.6130\n",
            "Epoch: 191, Loss: 0.1921, val_error: 0.6059\n",
            "Epoch: 192, Loss: 0.1437, val_error: 0.6186\n",
            "Epoch: 193, Loss: 0.1707, val_error: 0.6317\n",
            "Epoch: 194, Loss: 0.2108, val_error: 0.7044\n",
            "Epoch: 195, Loss: 0.1505, val_error: 0.6683\n",
            "Epoch: 196, Loss: 0.1878, val_error: 0.7096\n",
            "Epoch: 197, Loss: 0.1738, val_error: 0.7135\n",
            "Epoch: 198, Loss: 0.2094, val_error: 0.7243\n",
            "Epoch: 199, Loss: 0.1263, val_error: 0.7449\n",
            "Epoch: 200, Loss: 0.1506, val_error: 0.7155\n",
            "Epoch: 201, Loss: 0.1754, val_error: 0.7231\n",
            "Epoch: 202, Loss: 0.1338, val_error: 0.7016\n",
            "Epoch: 203, Loss: 0.1590, val_error: 0.6865\n",
            "Epoch: 204, Loss: 0.1306, val_error: 0.6671\n",
            "Epoch: 205, Loss: 0.1244, val_error: 0.6762\n",
            "Epoch: 206, Loss: 0.1724, val_error: 0.6762\n",
            "Epoch: 207, Loss: 0.1326, val_error: 0.6571\n",
            "Epoch: 208, Loss: 0.1910, val_error: 0.6420\n",
            "Epoch: 209, Loss: 0.1629, val_error: 0.5983\n",
            "Epoch: 210, Loss: 0.1570, val_error: 0.6575\n",
            "Epoch: 211, Loss: 0.1319, val_error: 0.6420\n",
            "Epoch: 212, Loss: 0.1862, val_error: 0.6452\n",
            "Epoch: 213, Loss: 0.1548, val_error: 0.6520\n",
            "Epoch: 214, Loss: 0.1278, val_error: 0.6393\n",
            "Epoch: 215, Loss: 0.1223, val_error: 0.6404\n",
            "Epoch: 216, Loss: 0.1505, val_error: 0.6841\n",
            "Epoch: 217, Loss: 0.1730, val_error: 0.7036\n",
            "Epoch: 218, Loss: 0.1454, val_error: 0.6901\n",
            "Epoch: 219, Loss: 0.1456, val_error: 0.6492\n",
            "Epoch: 220, Loss: 0.1491, val_error: 0.6977\n",
            "Epoch: 221, Loss: 0.1416, val_error: 0.6949\n",
            "Epoch: 222, Loss: 0.1631, val_error: 0.6834\n",
            "Epoch: 223, Loss: 0.1349, val_error: 0.6651\n",
            "Epoch: 224, Loss: 0.1351, val_error: 0.6341\n",
            "Epoch: 225, Loss: 0.1383, val_error: 0.6400\n",
            "Epoch: 226, Loss: 0.1452, val_error: 0.6130\n",
            "Epoch: 227, Loss: 0.1419, val_error: 0.6106\n",
            "Epoch: 228, Loss: 0.2047, val_error: 0.6146\n",
            "Epoch: 229, Loss: 0.1538, val_error: 0.6623\n",
            "Epoch: 230, Loss: 0.1638, val_error: 0.6444\n",
            "Epoch: 231, Loss: 0.1568, val_error: 0.6301\n",
            "Epoch: 232, Loss: 0.1340, val_error: 0.6476\n",
            "Epoch: 233, Loss: 0.1958, val_error: 0.7028\n",
            "Epoch: 234, Loss: 0.1363, val_error: 0.6845\n",
            "Epoch: 235, Loss: 0.1632, val_error: 0.6965\n",
            "Epoch: 236, Loss: 0.1930, val_error: 0.6416\n",
            "Epoch: 237, Loss: 0.1324, val_error: 0.6655\n",
            "Epoch: 238, Loss: 0.1543, val_error: 0.6671\n",
            "Epoch: 239, Loss: 0.1241, val_error: 0.6484\n",
            "Epoch: 240, Loss: 0.1843, val_error: 0.6202\n",
            "Epoch: 241, Loss: 0.1277, val_error: 0.6349\n",
            "Epoch: 242, Loss: 0.1309, val_error: 0.6587\n",
            "Epoch: 243, Loss: 0.2372, val_error: 0.6643\n",
            "Epoch: 244, Loss: 0.1647, val_error: 0.6309\n",
            "Epoch: 245, Loss: 0.1532, val_error: 0.6353\n",
            "Epoch: 246, Loss: 0.1407, val_error: 0.6698\n",
            "Epoch: 247, Loss: 0.1418, val_error: 0.6798\n",
            "Epoch: 248, Loss: 0.1227, val_error: 0.6742\n",
            "Epoch: 249, Loss: 0.1962, val_error: 0.6897\n",
            "Epoch: 250, Loss: 0.1405, val_error: 0.6830\n",
            "Epoch: 251, Loss: 0.1212, val_error: 0.6818\n",
            "Epoch: 252, Loss: 0.2096, val_error: 0.6865\n",
            "Epoch: 253, Loss: 0.1750, val_error: 0.7004\n",
            "Epoch: 254, Loss: 0.2162, val_error: 0.6810\n",
            "Epoch: 255, Loss: 0.1680, val_error: 0.6571\n",
            "Epoch: 256, Loss: 0.1374, val_error: 0.6834\n",
            "Epoch: 257, Loss: 0.1653, val_error: 0.7215\n",
            "Epoch: 258, Loss: 0.1286, val_error: 0.6925\n",
            "Epoch: 259, Loss: 0.1381, val_error: 0.6921\n",
            "Epoch: 260, Loss: 0.1328, val_error: 0.6901\n",
            "Epoch: 261, Loss: 0.1188, val_error: 0.6953\n",
            "Epoch: 262, Loss: 0.1294, val_error: 0.6766\n",
            "Epoch: 263, Loss: 0.1298, val_error: 0.7052\n",
            "Epoch: 264, Loss: 0.1185, val_error: 0.6555\n",
            "Epoch: 265, Loss: 0.1593, val_error: 0.6575\n",
            "Epoch: 266, Loss: 0.1347, val_error: 0.6547\n",
            "Epoch: 267, Loss: 0.1288, val_error: 0.6651\n",
            "Epoch: 268, Loss: 0.1271, val_error: 0.6559\n",
            "Epoch: 269, Loss: 0.1190, val_error: 0.6782\n",
            "Epoch: 270, Loss: 0.1481, val_error: 0.6397\n",
            "Epoch: 271, Loss: 0.1294, val_error: 0.6790\n",
            "Epoch: 272, Loss: 0.1176, val_error: 0.6559\n",
            "Epoch: 273, Loss: 0.1450, val_error: 0.6877\n",
            "Epoch: 274, Loss: 0.1662, val_error: 0.7132\n",
            "Epoch: 275, Loss: 0.1220, val_error: 0.7120\n",
            "Epoch: 276, Loss: 0.1419, val_error: 0.7195\n",
            "Epoch: 277, Loss: 0.1721, val_error: 0.7203\n",
            "Epoch: 278, Loss: 0.1104, val_error: 0.6881\n",
            "Epoch: 279, Loss: 0.1167, val_error: 0.7314\n",
            "Epoch: 280, Loss: 0.1423, val_error: 0.6826\n",
            "Epoch: 281, Loss: 0.1782, val_error: 0.6845\n",
            "Epoch: 282, Loss: 0.1447, val_error: 0.6917\n",
            "Epoch: 283, Loss: 0.1337, val_error: 0.6996\n",
            "Epoch: 284, Loss: 0.1142, val_error: 0.6496\n",
            "Epoch: 285, Loss: 0.1505, val_error: 0.6623\n",
            "Epoch: 286, Loss: 0.1362, val_error: 0.7004\n",
            "Epoch: 287, Loss: 0.1539, val_error: 0.6540\n",
            "Epoch: 288, Loss: 0.1354, val_error: 0.6436\n",
            "Epoch: 289, Loss: 0.1709, val_error: 0.6544\n",
            "Epoch: 290, Loss: 0.1367, val_error: 0.6532\n",
            "Epoch: 291, Loss: 0.1209, val_error: 0.6508\n",
            "Epoch: 292, Loss: 0.1128, val_error: 0.6706\n",
            "Epoch: 293, Loss: 0.1455, val_error: 0.6953\n",
            "Epoch: 294, Loss: 0.1241, val_error: 0.6679\n",
            "Epoch: 295, Loss: 0.1324, val_error: 0.6857\n",
            "Epoch: 296, Loss: 0.1223, val_error: 0.6913\n",
            "Epoch: 297, Loss: 0.1668, val_error: 0.7000\n",
            "Epoch: 298, Loss: 0.1370, val_error: 0.6865\n",
            "Epoch: 299, Loss: 0.1197, val_error: 0.6750\n",
            "Epoch: 300, Loss: 0.1383, val_error: 0.6325\n",
            "Epoch: 301, Loss: 0.2000, val_error: 0.6333\n",
            "Epoch: 302, Loss: 0.1516, val_error: 0.6110\n",
            "Epoch: 303, Loss: 0.1408, val_error: 0.6373\n",
            "Epoch: 304, Loss: 0.1496, val_error: 0.6571\n",
            "Epoch: 305, Loss: 0.1284, val_error: 0.6158\n",
            "Epoch: 306, Loss: 0.2749, val_error: 0.6297\n",
            "Epoch: 307, Loss: 0.1234, val_error: 0.6186\n",
            "Epoch: 308, Loss: 0.1249, val_error: 0.6015\n",
            "Epoch: 309, Loss: 0.1443, val_error: 0.6095\n",
            "Epoch: 310, Loss: 0.1267, val_error: 0.6460\n",
            "Epoch: 311, Loss: 0.1368, val_error: 0.6162\n",
            "Epoch: 312, Loss: 0.1840, val_error: 0.6432\n",
            "Epoch: 313, Loss: 0.1230, val_error: 0.6540\n",
            "Epoch: 314, Loss: 0.2246, val_error: 0.6436\n",
            "Epoch: 315, Loss: 0.1393, val_error: 0.6432\n",
            "Epoch: 316, Loss: 0.1584, val_error: 0.6500\n",
            "Epoch: 317, Loss: 0.1220, val_error: 0.6011\n",
            "Epoch: 318, Loss: 0.1347, val_error: 0.6257\n",
            "Epoch: 319, Loss: 0.1317, val_error: 0.6146\n",
            "Epoch: 320, Loss: 0.1234, val_error: 0.6349\n",
            "Epoch: 321, Loss: 0.1255, val_error: 0.6007\n",
            "Epoch: 322, Loss: 0.1268, val_error: 0.6377\n",
            "Epoch: 323, Loss: 0.1668, val_error: 0.6377\n",
            "Epoch: 324, Loss: 0.1227, val_error: 0.6345\n",
            "Epoch: 325, Loss: 0.1519, val_error: 0.6512\n",
            "Epoch: 326, Loss: 0.1319, val_error: 0.6313\n",
            "Epoch: 327, Loss: 0.1934, val_error: 0.6512\n",
            "Epoch: 328, Loss: 0.1370, val_error: 0.6623\n",
            "Epoch: 329, Loss: 0.1302, val_error: 0.6353\n",
            "Epoch: 330, Loss: 0.1312, val_error: 0.6285\n",
            "Epoch: 331, Loss: 0.1732, val_error: 0.6774\n",
            "Epoch: 332, Loss: 0.1304, val_error: 0.6655\n",
            "Epoch: 333, Loss: 0.1400, val_error: 0.6818\n",
            "Epoch: 334, Loss: 0.1278, val_error: 0.6933\n",
            "Epoch: 335, Loss: 0.1558, val_error: 0.6865\n",
            "Epoch: 336, Loss: 0.1235, val_error: 0.6806\n",
            "Epoch: 337, Loss: 0.1168, val_error: 0.6583\n",
            "Epoch: 338, Loss: 0.1458, val_error: 0.6341\n",
            "Epoch: 339, Loss: 0.1343, val_error: 0.6349\n",
            "Epoch: 340, Loss: 0.1345, val_error: 0.6210\n",
            "Epoch: 341, Loss: 0.1311, val_error: 0.6019\n",
            "Epoch: 342, Loss: 0.1175, val_error: 0.5991\n",
            "Epoch: 343, Loss: 0.1163, val_error: 0.6011\n",
            "Epoch: 344, Loss: 0.1431, val_error: 0.6035\n",
            "Epoch: 345, Loss: 0.1403, val_error: 0.6341\n",
            "Epoch: 346, Loss: 0.1196, val_error: 0.6313\n",
            "Epoch: 347, Loss: 0.1151, val_error: 0.6369\n",
            "Epoch: 348, Loss: 0.1120, val_error: 0.6265\n",
            "Epoch: 349, Loss: 0.1313, val_error: 0.6162\n",
            "Epoch: 350, Loss: 0.1556, val_error: 0.6508\n",
            "Epoch: 351, Loss: 0.2078, val_error: 0.6631\n",
            "Epoch: 352, Loss: 0.1158, val_error: 0.6853\n",
            "Epoch: 353, Loss: 0.1054, val_error: 0.6663\n",
            "Epoch: 354, Loss: 0.1853, val_error: 0.6969\n",
            "Epoch: 355, Loss: 0.1224, val_error: 0.7147\n",
            "Epoch: 356, Loss: 0.1154, val_error: 0.6877\n",
            "Epoch: 357, Loss: 0.1250, val_error: 0.6754\n",
            "Epoch: 358, Loss: 0.1555, val_error: 0.6659\n",
            "Epoch: 359, Loss: 0.1355, val_error: 0.6635\n",
            "Epoch: 360, Loss: 0.1269, val_error: 0.6524\n",
            "Epoch: 361, Loss: 0.2233, val_error: 0.6424\n",
            "Epoch: 362, Loss: 0.1181, val_error: 0.6186\n",
            "Epoch: 363, Loss: 0.1231, val_error: 0.5956\n",
            "Epoch: 364, Loss: 0.1505, val_error: 0.5864\n",
            "Epoch: 365, Loss: 0.1405, val_error: 0.5805\n",
            "Epoch: 366, Loss: 0.1316, val_error: 0.5805\n",
            "Epoch: 367, Loss: 0.1307, val_error: 0.5824\n",
            "Epoch: 368, Loss: 0.1238, val_error: 0.6261\n",
            "Epoch: 369, Loss: 0.1053, val_error: 0.6563\n",
            "Epoch: 370, Loss: 0.1466, val_error: 0.6750\n",
            "Epoch: 371, Loss: 0.2139, val_error: 0.6762\n",
            "Epoch: 372, Loss: 0.1300, val_error: 0.6857\n",
            "Epoch: 373, Loss: 0.1526, val_error: 0.6988\n",
            "Epoch: 374, Loss: 0.1259, val_error: 0.6790\n",
            "Epoch: 375, Loss: 0.1390, val_error: 0.7151\n",
            "Epoch: 376, Loss: 0.1202, val_error: 0.6893\n",
            "Epoch: 377, Loss: 0.1283, val_error: 0.7120\n",
            "Epoch: 378, Loss: 0.1191, val_error: 0.6981\n",
            "Epoch: 379, Loss: 0.1225, val_error: 0.6571\n",
            "Epoch: 380, Loss: 0.1400, val_error: 0.6718\n",
            "Epoch: 381, Loss: 0.1265, val_error: 0.6484\n",
            "Epoch: 382, Loss: 0.1557, val_error: 0.6929\n",
            "Epoch: 383, Loss: 0.1132, val_error: 0.6941\n",
            "Epoch: 384, Loss: 0.1294, val_error: 0.6929\n",
            "Epoch: 385, Loss: 0.2631, val_error: 0.6988\n",
            "Epoch: 386, Loss: 0.1032, val_error: 0.6746\n",
            "Epoch: 387, Loss: 0.1638, val_error: 0.6683\n",
            "Epoch: 388, Loss: 0.1538, val_error: 0.6341\n",
            "Epoch: 389, Loss: 0.1607, val_error: 0.6524\n",
            "Epoch: 390, Loss: 0.1848, val_error: 0.6138\n",
            "Epoch: 391, Loss: 0.1197, val_error: 0.6448\n",
            "Epoch: 392, Loss: 0.1334, val_error: 0.5908\n",
            "Epoch: 393, Loss: 0.1346, val_error: 0.5884\n",
            "Epoch: 394, Loss: 0.1269, val_error: 0.6079\n",
            "Epoch: 395, Loss: 0.1479, val_error: 0.6361\n",
            "Epoch: 396, Loss: 0.1259, val_error: 0.6567\n",
            "Epoch: 397, Loss: 0.1271, val_error: 0.6504\n",
            "Epoch: 398, Loss: 0.1431, val_error: 0.6996\n",
            "Epoch: 399, Loss: 0.1667, val_error: 0.6408\n",
            "Epoch: 400, Loss: 0.1408, val_error: 0.7163\n",
            "Epoch: 401, Loss: 0.1286, val_error: 0.6857\n",
            "Epoch: 402, Loss: 0.1208, val_error: 0.6845\n",
            "Epoch: 403, Loss: 0.1381, val_error: 0.6730\n",
            "Epoch: 404, Loss: 0.1121, val_error: 0.7263\n",
            "Epoch: 405, Loss: 0.1519, val_error: 0.6826\n",
            "Epoch: 406, Loss: 0.1658, val_error: 0.6977\n",
            "Epoch: 407, Loss: 0.1294, val_error: 0.7068\n",
            "Epoch: 408, Loss: 0.1247, val_error: 0.6865\n",
            "Epoch: 409, Loss: 0.1375, val_error: 0.6897\n",
            "Epoch: 410, Loss: 0.1204, val_error: 0.6913\n",
            "Epoch: 411, Loss: 0.1192, val_error: 0.7064\n",
            "Epoch: 412, Loss: 0.1819, val_error: 0.6786\n",
            "Epoch: 413, Loss: 0.1342, val_error: 0.6567\n",
            "Epoch: 414, Loss: 0.1060, val_error: 0.6885\n",
            "Epoch: 415, Loss: 0.1080, val_error: 0.6937\n",
            "Epoch: 416, Loss: 0.1073, val_error: 0.6834\n",
            "Epoch: 417, Loss: 0.1076, val_error: 0.6675\n",
            "Epoch: 418, Loss: 0.1086, val_error: 0.6742\n",
            "Epoch: 419, Loss: 0.1186, val_error: 0.6663\n",
            "Epoch: 420, Loss: 0.1174, val_error: 0.6687\n",
            "Epoch: 421, Loss: 0.1126, val_error: 0.6702\n",
            "Epoch: 422, Loss: 0.1376, val_error: 0.6838\n",
            "Epoch: 423, Loss: 0.1145, val_error: 0.6639\n",
            "Epoch: 424, Loss: 0.1227, val_error: 0.6412\n",
            "Epoch: 425, Loss: 0.1072, val_error: 0.6583\n",
            "Epoch: 426, Loss: 0.1070, val_error: 0.6472\n",
            "Epoch: 427, Loss: 0.1194, val_error: 0.6202\n",
            "Epoch: 428, Loss: 0.1602, val_error: 0.6925\n",
            "Epoch: 429, Loss: 0.1268, val_error: 0.6691\n",
            "Epoch: 430, Loss: 0.1429, val_error: 0.7044\n",
            "Epoch: 431, Loss: 0.1101, val_error: 0.6667\n",
            "Epoch: 432, Loss: 0.1201, val_error: 0.6698\n",
            "Epoch: 433, Loss: 0.1165, val_error: 0.6599\n",
            "Epoch: 434, Loss: 0.1034, val_error: 0.6691\n",
            "Epoch: 435, Loss: 0.1804, val_error: 0.6814\n",
            "Epoch: 436, Loss: 0.1168, val_error: 0.6416\n",
            "Epoch: 437, Loss: 0.1048, val_error: 0.6687\n",
            "Epoch: 438, Loss: 0.1280, val_error: 0.6222\n",
            "Epoch: 439, Loss: 0.1143, val_error: 0.6015\n",
            "Epoch: 440, Loss: 0.1283, val_error: 0.6404\n",
            "Epoch: 441, Loss: 0.1250, val_error: 0.6555\n",
            "Epoch: 442, Loss: 0.1005, val_error: 0.6468\n",
            "Epoch: 443, Loss: 0.1535, val_error: 0.6436\n",
            "Epoch: 444, Loss: 0.1085, val_error: 0.6544\n",
            "Epoch: 445, Loss: 0.1255, val_error: 0.6357\n",
            "Epoch: 446, Loss: 0.1051, val_error: 0.6623\n",
            "Epoch: 447, Loss: 0.1092, val_error: 0.6591\n",
            "Epoch: 448, Loss: 0.1226, val_error: 0.6520\n",
            "Epoch: 449, Loss: 0.1032, val_error: 0.6667\n",
            "Epoch: 450, Loss: 0.1246, val_error: 0.6563\n",
            "Epoch: 451, Loss: 0.1092, val_error: 0.6722\n",
            "Epoch: 452, Loss: 0.1081, val_error: 0.6571\n",
            "Epoch: 453, Loss: 0.1172, val_error: 0.6873\n",
            "Epoch: 454, Loss: 0.1116, val_error: 0.6540\n",
            "Epoch: 455, Loss: 0.1011, val_error: 0.6834\n",
            "Epoch: 456, Loss: 0.1154, val_error: 0.6687\n",
            "Epoch: 457, Loss: 0.1097, val_error: 0.6702\n",
            "Epoch: 458, Loss: 0.0995, val_error: 0.6969\n",
            "Epoch: 459, Loss: 0.1213, val_error: 0.6349\n",
            "Epoch: 460, Loss: 0.1085, val_error: 0.6691\n",
            "Epoch: 461, Loss: 0.1089, val_error: 0.6587\n",
            "Epoch: 462, Loss: 0.1064, val_error: 0.6583\n",
            "Epoch: 463, Loss: 0.0972, val_error: 0.6587\n",
            "Epoch: 464, Loss: 0.1193, val_error: 0.6377\n",
            "Epoch: 465, Loss: 0.1156, val_error: 0.6250\n",
            "Epoch: 466, Loss: 0.1042, val_error: 0.6595\n",
            "Epoch: 467, Loss: 0.1485, val_error: 0.6718\n",
            "Epoch: 468, Loss: 0.0991, val_error: 0.6770\n",
            "Epoch: 469, Loss: 0.1039, val_error: 0.6754\n",
            "Epoch: 470, Loss: 0.1244, val_error: 0.6818\n",
            "Epoch: 471, Loss: 0.1021, val_error: 0.6381\n",
            "Epoch: 472, Loss: 0.1567, val_error: 0.6226\n",
            "Epoch: 473, Loss: 0.1855, val_error: 0.6361\n",
            "Epoch: 474, Loss: 0.1137, val_error: 0.6210\n",
            "Epoch: 475, Loss: 0.1099, val_error: 0.6114\n",
            "Epoch: 476, Loss: 0.1023, val_error: 0.6444\n",
            "Epoch: 477, Loss: 0.1269, val_error: 0.6424\n",
            "Epoch: 478, Loss: 0.1101, val_error: 0.6615\n",
            "Epoch: 479, Loss: 0.1503, val_error: 0.6857\n",
            "Epoch: 480, Loss: 0.1038, val_error: 0.6778\n",
            "Epoch: 481, Loss: 0.0968, val_error: 0.7004\n",
            "Epoch: 482, Loss: 0.1188, val_error: 0.6865\n",
            "Epoch: 483, Loss: 0.0909, val_error: 0.7096\n",
            "Epoch: 484, Loss: 0.1243, val_error: 0.6782\n",
            "Epoch: 485, Loss: 0.1462, val_error: 0.6925\n",
            "Epoch: 486, Loss: 0.1147, val_error: 0.6698\n",
            "Epoch: 487, Loss: 0.1075, val_error: 0.6754\n",
            "Epoch: 488, Loss: 0.2530, val_error: 0.6460\n",
            "Epoch: 489, Loss: 0.1183, val_error: 0.6234\n",
            "Epoch: 490, Loss: 0.0999, val_error: 0.6750\n",
            "Epoch: 491, Loss: 0.1791, val_error: 0.6420\n",
            "Epoch: 492, Loss: 0.1096, val_error: 0.6381\n",
            "Epoch: 493, Loss: 0.1030, val_error: 0.6369\n",
            "Epoch: 494, Loss: 0.1052, val_error: 0.6420\n",
            "Epoch: 495, Loss: 0.1045, val_error: 0.6730\n",
            "Epoch: 496, Loss: 0.1298, val_error: 0.6806\n",
            "Epoch: 497, Loss: 0.1139, val_error: 0.6782\n",
            "Epoch: 498, Loss: 0.1159, val_error: 0.6742\n",
            "Epoch: 499, Loss: 0.1061, val_error: 0.6758\n",
            "Epoch: 500, Loss: 0.0967, val_error: 0.6861\n",
            "Epoch: 501, Loss: 0.1097, val_error: 0.6512\n",
            "Epoch: 502, Loss: 0.1005, val_error: 0.6786\n",
            "Epoch: 503, Loss: 0.1381, val_error: 0.6742\n",
            "Epoch: 504, Loss: 0.1030, val_error: 0.6718\n",
            "Epoch: 505, Loss: 0.1169, val_error: 0.6611\n",
            "Epoch: 506, Loss: 0.1301, val_error: 0.6440\n",
            "Epoch: 507, Loss: 0.1007, val_error: 0.6579\n",
            "Epoch: 508, Loss: 0.1089, val_error: 0.6603\n",
            "Epoch: 509, Loss: 0.1264, val_error: 0.6683\n",
            "Epoch: 510, Loss: 0.0990, val_error: 0.6742\n",
            "Epoch: 511, Loss: 0.1175, val_error: 0.6524\n",
            "Epoch: 512, Loss: 0.1018, val_error: 0.6766\n",
            "Epoch: 513, Loss: 0.1290, val_error: 0.6921\n",
            "Epoch: 514, Loss: 0.1200, val_error: 0.6865\n",
            "Epoch: 515, Loss: 0.0906, val_error: 0.6933\n",
            "Epoch: 516, Loss: 0.1009, val_error: 0.7203\n",
            "Epoch: 517, Loss: 0.0971, val_error: 0.7020\n",
            "Epoch: 518, Loss: 0.1168, val_error: 0.6996\n",
            "Epoch: 519, Loss: 0.1401, val_error: 0.6869\n",
            "Epoch: 520, Loss: 0.1018, val_error: 0.6579\n",
            "Epoch: 521, Loss: 0.2502, val_error: 0.6289\n",
            "Epoch: 522, Loss: 0.1334, val_error: 0.6432\n",
            "Epoch: 523, Loss: 0.1241, val_error: 0.6031\n",
            "Epoch: 524, Loss: 0.1073, val_error: 0.5745\n",
            "Epoch: 525, Loss: 0.1229, val_error: 0.5860\n",
            "Epoch: 526, Loss: 0.1115, val_error: 0.6341\n",
            "Epoch: 527, Loss: 0.1323, val_error: 0.6222\n",
            "Epoch: 528, Loss: 0.0997, val_error: 0.6547\n",
            "Epoch: 529, Loss: 0.1218, val_error: 0.6798\n",
            "Epoch: 530, Loss: 0.1256, val_error: 0.6571\n",
            "Epoch: 531, Loss: 0.1049, val_error: 0.6977\n",
            "Epoch: 532, Loss: 0.1031, val_error: 0.7247\n",
            "Epoch: 533, Loss: 0.1043, val_error: 0.7215\n",
            "Epoch: 534, Loss: 0.0984, val_error: 0.7243\n",
            "Epoch: 535, Loss: 0.1036, val_error: 0.7310\n",
            "Epoch: 536, Loss: 0.1059, val_error: 0.7235\n",
            "Epoch: 537, Loss: 0.1124, val_error: 0.6992\n",
            "Epoch: 538, Loss: 0.0975, val_error: 0.6905\n",
            "Epoch: 539, Loss: 0.1273, val_error: 0.6544\n",
            "Epoch: 540, Loss: 0.1154, val_error: 0.6599\n",
            "Epoch: 541, Loss: 0.1262, val_error: 0.6472\n",
            "Epoch: 542, Loss: 0.1013, val_error: 0.6440\n",
            "Epoch: 543, Loss: 0.1005, val_error: 0.6496\n",
            "Epoch: 544, Loss: 0.1073, val_error: 0.6448\n",
            "Epoch: 545, Loss: 0.1085, val_error: 0.6397\n",
            "Epoch: 546, Loss: 0.1779, val_error: 0.6063\n",
            "Epoch: 547, Loss: 0.1022, val_error: 0.6639\n",
            "Epoch: 548, Loss: 0.1164, val_error: 0.6643\n",
            "Epoch: 549, Loss: 0.1009, val_error: 0.6861\n",
            "Epoch: 550, Loss: 0.0903, val_error: 0.6806\n",
            "Epoch: 551, Loss: 0.1242, val_error: 0.6865\n",
            "Epoch: 552, Loss: 0.1131, val_error: 0.6969\n",
            "Epoch: 553, Loss: 0.1092, val_error: 0.7215\n",
            "Epoch: 554, Loss: 0.1333, val_error: 0.6941\n",
            "Epoch: 555, Loss: 0.0920, val_error: 0.6675\n",
            "Epoch: 556, Loss: 0.0974, val_error: 0.6786\n",
            "Epoch: 557, Loss: 0.1707, val_error: 0.6921\n",
            "Epoch: 558, Loss: 0.1090, val_error: 0.6873\n",
            "Epoch: 559, Loss: 0.0953, val_error: 0.6770\n",
            "Epoch: 560, Loss: 0.1255, val_error: 0.6838\n",
            "Epoch: 561, Loss: 0.0999, val_error: 0.6786\n",
            "Epoch: 562, Loss: 0.0972, val_error: 0.6524\n",
            "Epoch: 563, Loss: 0.2219, val_error: 0.6841\n",
            "Epoch: 564, Loss: 0.1082, val_error: 0.6603\n",
            "Epoch: 565, Loss: 0.0906, val_error: 0.6583\n",
            "Epoch: 566, Loss: 0.0985, val_error: 0.6476\n",
            "Epoch: 567, Loss: 0.1024, val_error: 0.6977\n",
            "Epoch: 568, Loss: 0.1227, val_error: 0.6841\n",
            "Epoch: 569, Loss: 0.1121, val_error: 0.6841\n",
            "Epoch: 570, Loss: 0.1167, val_error: 0.7072\n",
            "Epoch: 571, Loss: 0.1017, val_error: 0.6651\n",
            "Epoch: 572, Loss: 0.1132, val_error: 0.7024\n",
            "Epoch: 573, Loss: 0.1077, val_error: 0.6969\n",
            "Epoch: 574, Loss: 0.1275, val_error: 0.6647\n",
            "Epoch: 575, Loss: 0.1239, val_error: 0.7004\n",
            "Epoch: 576, Loss: 0.1026, val_error: 0.7235\n",
            "Epoch: 577, Loss: 0.1215, val_error: 0.7453\n",
            "Epoch: 578, Loss: 0.1248, val_error: 0.7362\n",
            "Epoch: 579, Loss: 0.1258, val_error: 0.7390\n",
            "Epoch: 580, Loss: 0.1103, val_error: 0.6957\n",
            "Epoch: 581, Loss: 0.0938, val_error: 0.7215\n",
            "Epoch: 582, Loss: 0.1281, val_error: 0.6448\n",
            "Epoch: 583, Loss: 0.0923, val_error: 0.7016\n",
            "Epoch: 584, Loss: 0.1017, val_error: 0.6826\n",
            "Epoch: 585, Loss: 0.1032, val_error: 0.6285\n",
            "Epoch: 586, Loss: 0.0846, val_error: 0.6734\n",
            "Epoch: 587, Loss: 0.1183, val_error: 0.6257\n",
            "Epoch: 588, Loss: 0.1391, val_error: 0.6571\n",
            "Epoch: 589, Loss: 0.1026, val_error: 0.6385\n",
            "Epoch: 590, Loss: 0.1120, val_error: 0.6424\n",
            "Epoch: 591, Loss: 0.0884, val_error: 0.7056\n",
            "Epoch: 592, Loss: 0.1801, val_error: 0.6635\n",
            "Epoch: 593, Loss: 0.1052, val_error: 0.6798\n",
            "Epoch: 594, Loss: 0.1277, val_error: 0.6889\n",
            "Epoch: 595, Loss: 0.0840, val_error: 0.7271\n",
            "Epoch: 596, Loss: 0.0937, val_error: 0.6913\n",
            "Epoch: 597, Loss: 0.1245, val_error: 0.7155\n",
            "Epoch: 598, Loss: 0.1008, val_error: 0.7147\n",
            "Epoch: 599, Loss: 0.1019, val_error: 0.6929\n",
            "Epoch: 600, Loss: 0.0997, val_error: 0.6869\n",
            "Epoch: 601, Loss: 0.1109, val_error: 0.6889\n",
            "Epoch: 602, Loss: 0.0882, val_error: 0.7008\n",
            "Epoch: 603, Loss: 0.0972, val_error: 0.6996\n",
            "Epoch: 604, Loss: 0.1175, val_error: 0.7139\n",
            "Epoch: 605, Loss: 0.1053, val_error: 0.6929\n",
            "Epoch: 606, Loss: 0.0936, val_error: 0.6782\n",
            "Epoch: 607, Loss: 0.0992, val_error: 0.7004\n",
            "Epoch: 608, Loss: 0.0952, val_error: 0.6702\n",
            "Epoch: 609, Loss: 0.0910, val_error: 0.6901\n",
            "Epoch: 610, Loss: 0.0833, val_error: 0.7147\n",
            "Epoch: 611, Loss: 0.0886, val_error: 0.6977\n",
            "Epoch: 612, Loss: 0.1669, val_error: 0.7231\n",
            "Epoch: 613, Loss: 0.0969, val_error: 0.7143\n",
            "Epoch: 614, Loss: 0.0955, val_error: 0.6857\n",
            "Epoch: 615, Loss: 0.1128, val_error: 0.7171\n",
            "Epoch: 616, Loss: 0.0939, val_error: 0.7092\n",
            "Epoch: 617, Loss: 0.0921, val_error: 0.7032\n",
            "Epoch: 618, Loss: 0.1098, val_error: 0.7155\n",
            "Epoch: 619, Loss: 0.1352, val_error: 0.7255\n",
            "Epoch: 620, Loss: 0.0836, val_error: 0.7151\n",
            "Epoch: 621, Loss: 0.1088, val_error: 0.7024\n",
            "Epoch: 622, Loss: 0.1164, val_error: 0.7175\n",
            "Epoch: 623, Loss: 0.1254, val_error: 0.7358\n",
            "Epoch: 624, Loss: 0.0981, val_error: 0.7080\n",
            "Epoch: 625, Loss: 0.1177, val_error: 0.7116\n",
            "Epoch: 626, Loss: 0.1054, val_error: 0.7052\n",
            "Epoch: 627, Loss: 0.1100, val_error: 0.7104\n",
            "Epoch: 628, Loss: 0.0917, val_error: 0.7310\n",
            "Epoch: 629, Loss: 0.0976, val_error: 0.7056\n",
            "Epoch: 630, Loss: 0.0987, val_error: 0.7124\n",
            "Epoch: 631, Loss: 0.0959, val_error: 0.6738\n",
            "Epoch: 632, Loss: 0.1009, val_error: 0.6961\n",
            "Epoch: 633, Loss: 0.1068, val_error: 0.6826\n",
            "Epoch: 634, Loss: 0.1005, val_error: 0.6484\n",
            "Epoch: 635, Loss: 0.0846, val_error: 0.6877\n",
            "Epoch: 636, Loss: 0.1164, val_error: 0.6961\n",
            "Epoch: 637, Loss: 0.0935, val_error: 0.6949\n",
            "Epoch: 638, Loss: 0.1104, val_error: 0.6988\n",
            "Epoch: 639, Loss: 0.0943, val_error: 0.6849\n",
            "Epoch: 640, Loss: 0.0983, val_error: 0.7207\n",
            "Epoch: 641, Loss: 0.0844, val_error: 0.7104\n",
            "Epoch: 642, Loss: 0.1451, val_error: 0.7000\n",
            "Epoch: 643, Loss: 0.0985, val_error: 0.6834\n",
            "Epoch: 644, Loss: 0.0935, val_error: 0.7132\n",
            "Epoch: 645, Loss: 0.0863, val_error: 0.7219\n",
            "Epoch: 646, Loss: 0.0901, val_error: 0.6786\n",
            "Epoch: 647, Loss: 0.1026, val_error: 0.6671\n",
            "Epoch: 648, Loss: 0.1062, val_error: 0.6861\n",
            "Epoch: 649, Loss: 0.1698, val_error: 0.6774\n",
            "Epoch: 650, Loss: 0.1138, val_error: 0.6456\n",
            "Epoch: 651, Loss: 0.0832, val_error: 0.6770\n",
            "Epoch: 652, Loss: 0.1097, val_error: 0.6885\n",
            "Epoch: 653, Loss: 0.1037, val_error: 0.6790\n",
            "Epoch: 654, Loss: 0.1029, val_error: 0.6492\n",
            "Epoch: 655, Loss: 0.1136, val_error: 0.6802\n",
            "Epoch: 656, Loss: 0.0929, val_error: 0.6929\n",
            "Epoch: 657, Loss: 0.0830, val_error: 0.6857\n",
            "Epoch: 658, Loss: 0.0925, val_error: 0.6929\n",
            "Epoch: 659, Loss: 0.1001, val_error: 0.7151\n",
            "Epoch: 660, Loss: 0.0894, val_error: 0.7116\n",
            "Epoch: 661, Loss: 0.1795, val_error: 0.7120\n",
            "Epoch: 662, Loss: 0.1257, val_error: 0.7239\n",
            "Epoch: 663, Loss: 0.0909, val_error: 0.7159\n",
            "Epoch: 664, Loss: 0.0995, val_error: 0.7072\n",
            "Epoch: 665, Loss: 0.1067, val_error: 0.7128\n",
            "Epoch: 666, Loss: 0.1086, val_error: 0.6754\n",
            "Epoch: 667, Loss: 0.0935, val_error: 0.6782\n",
            "Epoch: 668, Loss: 0.1234, val_error: 0.7132\n",
            "Epoch: 669, Loss: 0.1023, val_error: 0.6623\n",
            "Epoch: 670, Loss: 0.0985, val_error: 0.6639\n",
            "Epoch: 671, Loss: 0.0999, val_error: 0.6961\n",
            "Epoch: 672, Loss: 0.1061, val_error: 0.7219\n",
            "Epoch: 673, Loss: 0.0871, val_error: 0.7108\n",
            "Epoch: 674, Loss: 0.0822, val_error: 0.7040\n",
            "Epoch: 675, Loss: 0.0907, val_error: 0.7116\n",
            "Epoch: 676, Loss: 0.0905, val_error: 0.7211\n",
            "Epoch: 677, Loss: 0.1022, val_error: 0.7004\n",
            "Epoch: 678, Loss: 0.0841, val_error: 0.6992\n",
            "Epoch: 679, Loss: 0.0883, val_error: 0.7271\n",
            "Epoch: 680, Loss: 0.0985, val_error: 0.7128\n",
            "Epoch: 681, Loss: 0.1431, val_error: 0.7163\n",
            "Epoch: 682, Loss: 0.0952, val_error: 0.7135\n",
            "Epoch: 683, Loss: 0.1240, val_error: 0.7314\n",
            "Epoch: 684, Loss: 0.1258, val_error: 0.7386\n",
            "Epoch: 685, Loss: 0.0919, val_error: 0.7406\n",
            "Epoch: 686, Loss: 0.1247, val_error: 0.7294\n",
            "Epoch: 687, Loss: 0.1030, val_error: 0.7394\n",
            "Epoch: 688, Loss: 0.1403, val_error: 0.7576\n",
            "Epoch: 689, Loss: 0.0880, val_error: 0.7565\n",
            "Epoch: 690, Loss: 0.1011, val_error: 0.7553\n",
            "Epoch: 691, Loss: 0.0942, val_error: 0.7426\n",
            "Epoch: 692, Loss: 0.1133, val_error: 0.7382\n",
            "Epoch: 693, Loss: 0.2136, val_error: 0.7553\n",
            "Epoch: 694, Loss: 0.1145, val_error: 0.7584\n",
            "Epoch: 695, Loss: 0.1022, val_error: 0.7576\n",
            "Epoch: 696, Loss: 0.1111, val_error: 0.7525\n",
            "Epoch: 697, Loss: 0.1090, val_error: 0.7549\n",
            "Epoch: 698, Loss: 0.0943, val_error: 0.7565\n",
            "Epoch: 699, Loss: 0.1011, val_error: 0.7600\n",
            "Epoch: 700, Loss: 0.0976, val_error: 0.7688\n",
            "Epoch: 701, Loss: 0.1052, val_error: 0.7644\n",
            "Epoch: 702, Loss: 0.1065, val_error: 0.7390\n",
            "Epoch: 703, Loss: 0.0926, val_error: 0.7088\n",
            "Epoch: 704, Loss: 0.1038, val_error: 0.7414\n",
            "Epoch: 705, Loss: 0.0981, val_error: 0.7155\n",
            "Epoch: 706, Loss: 0.0952, val_error: 0.7207\n",
            "Epoch: 707, Loss: 0.0868, val_error: 0.7016\n",
            "Epoch: 708, Loss: 0.0813, val_error: 0.7092\n",
            "Epoch: 709, Loss: 0.0916, val_error: 0.7000\n",
            "Epoch: 710, Loss: 0.0877, val_error: 0.7135\n",
            "Epoch: 711, Loss: 0.0883, val_error: 0.7426\n",
            "Epoch: 712, Loss: 0.1042, val_error: 0.7338\n",
            "Epoch: 713, Loss: 0.0920, val_error: 0.7298\n",
            "Epoch: 714, Loss: 0.0887, val_error: 0.7294\n",
            "Epoch: 715, Loss: 0.0864, val_error: 0.7243\n",
            "Epoch: 716, Loss: 0.0937, val_error: 0.7354\n",
            "Epoch: 717, Loss: 0.0990, val_error: 0.7406\n",
            "Epoch: 718, Loss: 0.0878, val_error: 0.7314\n",
            "Epoch: 719, Loss: 0.0869, val_error: 0.7477\n",
            "Epoch: 720, Loss: 0.1559, val_error: 0.7426\n",
            "Epoch: 721, Loss: 0.0949, val_error: 0.7580\n",
            "Epoch: 722, Loss: 0.0943, val_error: 0.7294\n",
            "Epoch: 723, Loss: 0.0897, val_error: 0.7461\n",
            "Epoch: 724, Loss: 0.1413, val_error: 0.7286\n",
            "Epoch: 725, Loss: 0.1343, val_error: 0.7159\n",
            "Epoch: 726, Loss: 0.1156, val_error: 0.7132\n",
            "Epoch: 727, Loss: 0.1012, val_error: 0.7648\n",
            "Epoch: 728, Loss: 0.1238, val_error: 0.7235\n",
            "Epoch: 729, Loss: 0.1059, val_error: 0.7322\n",
            "Epoch: 730, Loss: 0.1151, val_error: 0.7652\n",
            "Epoch: 731, Loss: 0.0964, val_error: 0.7358\n",
            "Epoch: 732, Loss: 0.0943, val_error: 0.6961\n",
            "Epoch: 733, Loss: 0.0996, val_error: 0.7215\n",
            "Epoch: 734, Loss: 0.1167, val_error: 0.7132\n",
            "Epoch: 735, Loss: 0.0870, val_error: 0.7151\n",
            "Epoch: 736, Loss: 0.0995, val_error: 0.7120\n",
            "Epoch: 737, Loss: 0.1449, val_error: 0.6977\n",
            "Epoch: 738, Loss: 0.1086, val_error: 0.6861\n",
            "Epoch: 739, Loss: 0.1679, val_error: 0.6822\n",
            "Epoch: 740, Loss: 0.1232, val_error: 0.6957\n",
            "Epoch: 741, Loss: 0.0821, val_error: 0.7036\n",
            "Epoch: 742, Loss: 0.0948, val_error: 0.7112\n",
            "Epoch: 743, Loss: 0.1018, val_error: 0.7020\n",
            "Epoch: 744, Loss: 0.1079, val_error: 0.7100\n",
            "Epoch: 745, Loss: 0.1078, val_error: 0.7426\n",
            "Epoch: 746, Loss: 0.1810, val_error: 0.7529\n",
            "Epoch: 747, Loss: 0.0875, val_error: 0.7557\n",
            "Epoch: 748, Loss: 0.1258, val_error: 0.7441\n",
            "Epoch: 749, Loss: 0.1184, val_error: 0.7076\n",
            "Epoch: 750, Loss: 0.1175, val_error: 0.7473\n",
            "Epoch: 751, Loss: 0.1187, val_error: 0.7374\n",
            "Epoch: 752, Loss: 0.1424, val_error: 0.7096\n",
            "Epoch: 753, Loss: 0.0977, val_error: 0.7489\n",
            "Epoch: 754, Loss: 0.1452, val_error: 0.7275\n",
            "Epoch: 755, Loss: 0.0927, val_error: 0.7501\n",
            "Epoch: 756, Loss: 0.1087, val_error: 0.7557\n",
            "Epoch: 757, Loss: 0.0839, val_error: 0.7394\n",
            "Epoch: 758, Loss: 0.0906, val_error: 0.7537\n",
            "Epoch: 759, Loss: 0.1032, val_error: 0.7422\n",
            "Epoch: 760, Loss: 0.1034, val_error: 0.7239\n",
            "Epoch: 761, Loss: 0.1269, val_error: 0.7433\n",
            "Epoch: 762, Loss: 0.1060, val_error: 0.7692\n",
            "Epoch: 763, Loss: 0.1439, val_error: 0.7569\n",
            "Epoch: 764, Loss: 0.1111, val_error: 0.7398\n",
            "Epoch: 765, Loss: 0.0996, val_error: 0.7247\n",
            "Epoch: 766, Loss: 0.1012, val_error: 0.7132\n",
            "Epoch: 767, Loss: 0.1782, val_error: 0.6996\n",
            "Epoch: 768, Loss: 0.1188, val_error: 0.6945\n",
            "Epoch: 769, Loss: 0.0968, val_error: 0.7052\n",
            "Epoch: 770, Loss: 0.0924, val_error: 0.7020\n",
            "Epoch: 771, Loss: 0.0951, val_error: 0.6762\n",
            "Epoch: 772, Loss: 0.0986, val_error: 0.6587\n",
            "Epoch: 773, Loss: 0.0992, val_error: 0.6786\n",
            "Epoch: 774, Loss: 0.1100, val_error: 0.6965\n",
            "Epoch: 775, Loss: 0.0969, val_error: 0.7048\n",
            "Epoch: 776, Loss: 0.1043, val_error: 0.7104\n",
            "Epoch: 777, Loss: 0.0996, val_error: 0.7410\n",
            "Epoch: 778, Loss: 0.0997, val_error: 0.7310\n",
            "Epoch: 779, Loss: 0.0842, val_error: 0.7513\n",
            "Epoch: 780, Loss: 0.0861, val_error: 0.7592\n",
            "Epoch: 781, Loss: 0.1731, val_error: 0.7410\n",
            "Epoch: 782, Loss: 0.0735, val_error: 0.7362\n",
            "Epoch: 783, Loss: 0.0927, val_error: 0.7187\n",
            "Epoch: 784, Loss: 0.1159, val_error: 0.7346\n",
            "Epoch: 785, Loss: 0.0922, val_error: 0.7108\n",
            "Epoch: 786, Loss: 0.1009, val_error: 0.7124\n",
            "Epoch: 787, Loss: 0.1209, val_error: 0.7179\n",
            "Epoch: 788, Loss: 0.0989, val_error: 0.7267\n",
            "Epoch: 789, Loss: 0.0923, val_error: 0.7211\n",
            "Epoch: 790, Loss: 0.0933, val_error: 0.7100\n",
            "Epoch: 791, Loss: 0.0848, val_error: 0.7143\n",
            "Epoch: 792, Loss: 0.1788, val_error: 0.7012\n",
            "Epoch: 793, Loss: 0.0907, val_error: 0.7239\n",
            "Epoch: 794, Loss: 0.0980, val_error: 0.6969\n",
            "Epoch: 795, Loss: 0.1152, val_error: 0.7068\n",
            "Epoch: 796, Loss: 0.1064, val_error: 0.7068\n",
            "Epoch: 797, Loss: 0.1345, val_error: 0.7195\n",
            "Epoch: 798, Loss: 0.0830, val_error: 0.7235\n",
            "Epoch: 799, Loss: 0.0964, val_error: 0.6996\n",
            "Epoch: 800, Loss: 0.0963, val_error: 0.7497\n",
            "Epoch: 801, Loss: 0.0881, val_error: 0.7545\n",
            "Epoch: 802, Loss: 0.0730, val_error: 0.7513\n",
            "Epoch: 803, Loss: 0.0989, val_error: 0.7716\n",
            "Epoch: 804, Loss: 0.0886, val_error: 0.7588\n",
            "Epoch: 805, Loss: 0.0887, val_error: 0.7298\n",
            "Epoch: 806, Loss: 0.0946, val_error: 0.7672\n",
            "Epoch: 807, Loss: 0.0882, val_error: 0.7394\n",
            "Epoch: 808, Loss: 0.1149, val_error: 0.7326\n",
            "Epoch: 809, Loss: 0.0839, val_error: 0.7342\n",
            "Epoch: 810, Loss: 0.0906, val_error: 0.7378\n",
            "Epoch: 811, Loss: 0.0904, val_error: 0.7318\n",
            "Epoch: 812, Loss: 0.0869, val_error: 0.7370\n",
            "Epoch: 813, Loss: 0.0890, val_error: 0.7163\n",
            "Epoch: 814, Loss: 0.0968, val_error: 0.7135\n",
            "Epoch: 815, Loss: 0.0920, val_error: 0.6762\n",
            "Epoch: 816, Loss: 0.0852, val_error: 0.7135\n",
            "Epoch: 817, Loss: 0.1102, val_error: 0.7199\n",
            "Epoch: 818, Loss: 0.0855, val_error: 0.7203\n",
            "Epoch: 819, Loss: 0.1033, val_error: 0.6921\n",
            "Epoch: 820, Loss: 0.0761, val_error: 0.7124\n",
            "Epoch: 821, Loss: 0.0863, val_error: 0.7521\n",
            "Epoch: 822, Loss: 0.0896, val_error: 0.7505\n",
            "Epoch: 823, Loss: 0.0919, val_error: 0.7616\n",
            "Epoch: 824, Loss: 0.1021, val_error: 0.7243\n",
            "Epoch: 825, Loss: 0.1292, val_error: 0.7199\n",
            "Epoch: 826, Loss: 0.0804, val_error: 0.7433\n",
            "Epoch: 827, Loss: 0.1131, val_error: 0.6869\n",
            "Epoch: 828, Loss: 0.0910, val_error: 0.6897\n",
            "Epoch: 829, Loss: 0.0857, val_error: 0.7227\n",
            "Epoch: 830, Loss: 0.0927, val_error: 0.7052\n",
            "Epoch: 831, Loss: 0.0836, val_error: 0.7183\n",
            "Epoch: 832, Loss: 0.0749, val_error: 0.7096\n",
            "Epoch: 833, Loss: 0.0967, val_error: 0.7072\n",
            "Epoch: 834, Loss: 0.0777, val_error: 0.6710\n",
            "Epoch: 835, Loss: 0.0983, val_error: 0.6857\n",
            "Epoch: 836, Loss: 0.0885, val_error: 0.6921\n",
            "Epoch: 837, Loss: 0.0875, val_error: 0.6726\n",
            "Epoch: 838, Loss: 0.1468, val_error: 0.6790\n",
            "Epoch: 839, Loss: 0.0935, val_error: 0.6619\n",
            "Epoch: 840, Loss: 0.0906, val_error: 0.6691\n",
            "Epoch: 841, Loss: 0.0956, val_error: 0.6611\n",
            "Epoch: 842, Loss: 0.0842, val_error: 0.6834\n",
            "Epoch: 843, Loss: 0.0833, val_error: 0.7096\n",
            "Epoch: 844, Loss: 0.1223, val_error: 0.7108\n",
            "Epoch: 845, Loss: 0.0887, val_error: 0.7422\n",
            "Epoch: 846, Loss: 0.0888, val_error: 0.7680\n",
            "Epoch: 847, Loss: 0.1080, val_error: 0.7692\n",
            "Epoch: 848, Loss: 0.1025, val_error: 0.7819\n",
            "Epoch: 849, Loss: 0.0723, val_error: 0.7855\n",
            "Epoch: 850, Loss: 0.0781, val_error: 0.7743\n",
            "Epoch: 851, Loss: 0.0835, val_error: 0.7676\n",
            "Epoch: 852, Loss: 0.0771, val_error: 0.7366\n",
            "Epoch: 853, Loss: 0.0805, val_error: 0.7112\n",
            "Epoch: 854, Loss: 0.0850, val_error: 0.7064\n",
            "Epoch: 855, Loss: 0.0965, val_error: 0.6988\n",
            "Epoch: 856, Loss: 0.0824, val_error: 0.7088\n",
            "Epoch: 857, Loss: 0.1232, val_error: 0.6857\n",
            "Epoch: 858, Loss: 0.0791, val_error: 0.7282\n",
            "Epoch: 859, Loss: 0.0864, val_error: 0.7155\n",
            "Epoch: 860, Loss: 0.1787, val_error: 0.7334\n",
            "Epoch: 861, Loss: 0.0878, val_error: 0.7501\n",
            "Epoch: 862, Loss: 0.1031, val_error: 0.7199\n",
            "Epoch: 863, Loss: 0.0819, val_error: 0.7247\n",
            "Epoch: 864, Loss: 0.0772, val_error: 0.7398\n",
            "Epoch: 865, Loss: 0.0832, val_error: 0.6889\n",
            "Epoch: 866, Loss: 0.0999, val_error: 0.7159\n",
            "Epoch: 867, Loss: 0.1093, val_error: 0.7215\n",
            "Epoch: 868, Loss: 0.0753, val_error: 0.7338\n",
            "Epoch: 869, Loss: 0.0795, val_error: 0.6961\n",
            "Epoch: 870, Loss: 0.0830, val_error: 0.7143\n",
            "Epoch: 871, Loss: 0.0880, val_error: 0.6925\n",
            "Epoch: 872, Loss: 0.0801, val_error: 0.7155\n",
            "Epoch: 873, Loss: 0.0841, val_error: 0.6996\n",
            "Epoch: 874, Loss: 0.0958, val_error: 0.6941\n",
            "Epoch: 875, Loss: 0.0667, val_error: 0.6996\n",
            "Epoch: 876, Loss: 0.0963, val_error: 0.7080\n",
            "Epoch: 877, Loss: 0.1985, val_error: 0.7004\n",
            "Epoch: 878, Loss: 0.1019, val_error: 0.7016\n",
            "Epoch: 879, Loss: 0.0887, val_error: 0.6929\n",
            "Epoch: 880, Loss: 0.1346, val_error: 0.6941\n",
            "Epoch: 881, Loss: 0.1570, val_error: 0.7112\n",
            "Epoch: 882, Loss: 0.0960, val_error: 0.7275\n",
            "Epoch: 883, Loss: 0.1121, val_error: 0.7108\n",
            "Epoch: 884, Loss: 0.1080, val_error: 0.6750\n",
            "Epoch: 885, Loss: 0.1072, val_error: 0.6818\n",
            "Epoch: 886, Loss: 0.0935, val_error: 0.6885\n",
            "Epoch: 887, Loss: 0.1693, val_error: 0.6917\n",
            "Epoch: 888, Loss: 0.1195, val_error: 0.6770\n",
            "Epoch: 889, Loss: 0.1099, val_error: 0.6619\n",
            "Epoch: 890, Loss: 0.1337, val_error: 0.6790\n",
            "Epoch: 891, Loss: 0.1083, val_error: 0.6595\n",
            "Epoch: 892, Loss: 0.1008, val_error: 0.6571\n",
            "Epoch: 893, Loss: 0.1054, val_error: 0.6563\n",
            "Epoch: 894, Loss: 0.1404, val_error: 0.6806\n",
            "Epoch: 895, Loss: 0.0969, val_error: 0.6301\n",
            "Epoch: 896, Loss: 0.1386, val_error: 0.6694\n",
            "Epoch: 897, Loss: 0.1032, val_error: 0.6818\n",
            "Epoch: 898, Loss: 0.1086, val_error: 0.6583\n",
            "Epoch: 899, Loss: 0.0966, val_error: 0.6857\n",
            "Epoch: 900, Loss: 0.1110, val_error: 0.6659\n",
            "Epoch: 901, Loss: 0.0967, val_error: 0.6754\n",
            "Epoch: 902, Loss: 0.0944, val_error: 0.6579\n",
            "Epoch: 903, Loss: 0.0902, val_error: 0.6631\n",
            "Epoch: 904, Loss: 0.0870, val_error: 0.6345\n",
            "Epoch: 905, Loss: 0.1009, val_error: 0.6798\n",
            "Epoch: 906, Loss: 0.1001, val_error: 0.6917\n",
            "Epoch: 907, Loss: 0.1132, val_error: 0.7064\n",
            "Epoch: 908, Loss: 0.0929, val_error: 0.6635\n",
            "Epoch: 909, Loss: 0.0780, val_error: 0.7358\n",
            "Epoch: 910, Loss: 0.0840, val_error: 0.7275\n",
            "Epoch: 911, Loss: 0.0792, val_error: 0.7143\n",
            "Epoch: 912, Loss: 0.1145, val_error: 0.6965\n",
            "Epoch: 913, Loss: 0.0904, val_error: 0.7227\n",
            "Epoch: 914, Loss: 0.1504, val_error: 0.7036\n",
            "Epoch: 915, Loss: 0.0871, val_error: 0.6786\n",
            "Epoch: 916, Loss: 0.0840, val_error: 0.6702\n",
            "Epoch: 917, Loss: 0.1013, val_error: 0.6810\n",
            "Epoch: 918, Loss: 0.1308, val_error: 0.6484\n",
            "Epoch: 919, Loss: 0.0870, val_error: 0.6444\n",
            "Epoch: 920, Loss: 0.0930, val_error: 0.6683\n",
            "Epoch: 921, Loss: 0.0834, val_error: 0.6901\n",
            "Epoch: 922, Loss: 0.0787, val_error: 0.7088\n",
            "Epoch: 923, Loss: 0.1014, val_error: 0.7286\n",
            "Epoch: 924, Loss: 0.0916, val_error: 0.7437\n",
            "Epoch: 925, Loss: 0.1002, val_error: 0.7549\n",
            "Epoch: 926, Loss: 0.0988, val_error: 0.7652\n",
            "Epoch: 927, Loss: 0.0855, val_error: 0.7751\n",
            "Epoch: 928, Loss: 0.0933, val_error: 0.7712\n",
            "Epoch: 929, Loss: 0.0854, val_error: 0.7775\n",
            "Epoch: 930, Loss: 0.0801, val_error: 0.7731\n",
            "Epoch: 931, Loss: 0.0855, val_error: 0.7219\n",
            "Epoch: 932, Loss: 0.0787, val_error: 0.7501\n",
            "Epoch: 933, Loss: 0.0969, val_error: 0.7437\n",
            "Epoch: 934, Loss: 0.0775, val_error: 0.7469\n",
            "Epoch: 935, Loss: 0.0810, val_error: 0.7561\n",
            "Epoch: 936, Loss: 0.1308, val_error: 0.7628\n",
            "Epoch: 937, Loss: 0.0805, val_error: 0.7700\n",
            "Epoch: 938, Loss: 0.0750, val_error: 0.7632\n",
            "Epoch: 939, Loss: 0.1057, val_error: 0.7648\n",
            "Epoch: 940, Loss: 0.0846, val_error: 0.7505\n",
            "Epoch: 941, Loss: 0.1010, val_error: 0.7672\n",
            "Epoch: 942, Loss: 0.0861, val_error: 0.7580\n",
            "Epoch: 943, Loss: 0.0788, val_error: 0.7477\n",
            "Epoch: 944, Loss: 0.0909, val_error: 0.7171\n",
            "Epoch: 945, Loss: 0.0910, val_error: 0.7573\n",
            "Epoch: 946, Loss: 0.0886, val_error: 0.7600\n",
            "Epoch: 947, Loss: 0.1002, val_error: 0.7124\n",
            "Epoch: 948, Loss: 0.0978, val_error: 0.7541\n",
            "Epoch: 949, Loss: 0.1093, val_error: 0.7501\n",
            "Epoch: 950, Loss: 0.0838, val_error: 0.7298\n",
            "Epoch: 951, Loss: 0.1140, val_error: 0.7437\n",
            "Epoch: 952, Loss: 0.0843, val_error: 0.7239\n",
            "Epoch: 953, Loss: 0.0860, val_error: 0.7326\n",
            "Epoch: 954, Loss: 0.0936, val_error: 0.7020\n",
            "Epoch: 955, Loss: 0.0948, val_error: 0.7251\n",
            "Epoch: 956, Loss: 0.0938, val_error: 0.7263\n",
            "Epoch: 957, Loss: 0.0998, val_error: 0.7342\n",
            "Epoch: 958, Loss: 0.0822, val_error: 0.7346\n",
            "Epoch: 959, Loss: 0.0848, val_error: 0.7374\n",
            "Epoch: 960, Loss: 0.0972, val_error: 0.7000\n",
            "Epoch: 961, Loss: 0.1202, val_error: 0.6754\n",
            "Epoch: 962, Loss: 0.0859, val_error: 0.7350\n",
            "Epoch: 963, Loss: 0.0814, val_error: 0.7259\n",
            "Epoch: 964, Loss: 0.0822, val_error: 0.6841\n",
            "Epoch: 965, Loss: 0.0836, val_error: 0.7199\n",
            "Epoch: 966, Loss: 0.1203, val_error: 0.7267\n",
            "Epoch: 967, Loss: 0.0772, val_error: 0.7171\n",
            "Epoch: 968, Loss: 0.0826, val_error: 0.7128\n",
            "Epoch: 969, Loss: 0.0896, val_error: 0.7143\n",
            "Epoch: 970, Loss: 0.0770, val_error: 0.7163\n",
            "Epoch: 971, Loss: 0.1009, val_error: 0.6893\n",
            "Epoch: 972, Loss: 0.0871, val_error: 0.6996\n",
            "Epoch: 973, Loss: 0.0710, val_error: 0.6790\n",
            "Epoch: 974, Loss: 0.0835, val_error: 0.6818\n",
            "Epoch: 975, Loss: 0.1001, val_error: 0.6905\n",
            "Epoch: 976, Loss: 0.0896, val_error: 0.6901\n",
            "Epoch: 977, Loss: 0.0682, val_error: 0.7120\n",
            "Epoch: 978, Loss: 0.0955, val_error: 0.7251\n",
            "Epoch: 979, Loss: 0.0749, val_error: 0.7298\n",
            "Epoch: 980, Loss: 0.0873, val_error: 0.7354\n",
            "Epoch: 981, Loss: 0.0962, val_error: 0.7350\n",
            "Epoch: 982, Loss: 0.0804, val_error: 0.7624\n",
            "Epoch: 983, Loss: 0.1181, val_error: 0.7263\n",
            "Epoch: 984, Loss: 0.0734, val_error: 0.7247\n",
            "Epoch: 985, Loss: 0.0921, val_error: 0.7251\n",
            "Epoch: 986, Loss: 0.0795, val_error: 0.7183\n",
            "Epoch: 987, Loss: 0.0829, val_error: 0.7207\n",
            "Epoch: 988, Loss: 0.0937, val_error: 0.7179\n",
            "Epoch: 989, Loss: 0.0967, val_error: 0.7195\n",
            "Epoch: 990, Loss: 0.0793, val_error: 0.7263\n",
            "Epoch: 991, Loss: 0.1010, val_error: 0.7318\n",
            "Epoch: 992, Loss: 0.0772, val_error: 0.7402\n",
            "Epoch: 993, Loss: 0.0890, val_error: 0.7553\n",
            "Epoch: 994, Loss: 0.0943, val_error: 0.7441\n",
            "Epoch: 995, Loss: 0.1071, val_error: 0.7437\n",
            "Epoch: 996, Loss: 0.0761, val_error: 0.7378\n",
            "Epoch: 997, Loss: 0.0926, val_error: 0.7282\n",
            "Epoch: 998, Loss: 0.0787, val_error: 0.7290\n",
            "Epoch: 999, Loss: 0.1172, val_error: 0.7211\n",
            "Epoch: 1000, Loss: 0.1594, val_error: 0.7151\n"
          ]
        }
      ],
      "source": [
        "from IPython.display import Javascript  # Restrict height of output cell.\n",
        "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
        "\n",
        "model = GAT(hidden_channels=16)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
        "#criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "def train():\n",
        "      model.train()\n",
        "      optimizer.zero_grad()  # Clear gradients.\n",
        "      out = model(graph.x, graph.edge_index)  # Perform a single forward pass.\n",
        "      loss = criterion(out[graph.train_mask], graph.y[graph.train_mask])  # Compute the loss solely based on the training nodes.\n",
        "      loss.backward()  # Derive gradients.\n",
        "      optimizer.step()  # Update parameters based on gradients.\n",
        "\n",
        "      # --- Validation error ---\n",
        "      model.eval()  # switch to eval mode (disables dropout, etc.)\n",
        "      with torch.no_grad():\n",
        "            pred = out.argmax(dim=1)\n",
        "            val_pred = pred[graph.val_mask]\n",
        "            val_true = graph.y[graph.val_mask]\n",
        "            val_acc = (val_pred == val_true).float().mean().item()\n",
        "    \n",
        "      return loss.item(), val_acc\n",
        "\n",
        "def test():\n",
        "      model.eval()\n",
        "      out = model(graph.x, graph.edge_index)\n",
        "      pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
        "      test_correct = pred[graph.test_mask] == graph.y[graph.test_mask]  # Check against ground-truth labels.\n",
        "      test_acc = int(test_correct.sum()) / int(graph.test_mask.sum())  # Derive ratio of correct predictions.\n",
        "      return test_acc\n",
        "\n",
        "\n",
        "for epoch in range(1, 1001):\n",
        "    loss, val_acc = train()\n",
        "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, val_error: {val_acc:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assume:\n",
        "# - y_true: ground truth labels of test nodes\n",
        "# - y_pred: predicted labels of test nodes\n",
        "\n",
        "# Example:\n",
        "y_pred = model(graph.x, graph.edge_index).argmax(dim=1)\n",
        "y_true = graph.y\n",
        "\n",
        "# Use the test mask to slice out test nodes\n",
        "y_pred_test = y_pred[graph.test_mask]\n",
        "y_true_test = y_true[graph.test_mask]\n",
        "\n",
        "# Classification report (includes precision, recall, F1 per class)\n",
        "report = classification_report(y_true_test.cpu(), y_pred_test.cpu(), digits=3, output_dict=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.7504512145764127"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#f1_weighted_GAT_GNG = report[\"weighted avg\"][\"f1-score\"]\n",
        "#f1_weighted_GAT_GNG\n",
        "f1_weighted_GAT_SMP_R = report[\"weighted avg\"][\"f1-score\"]\n",
        "f1_weighted_GAT_SMP_R"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot([f1_weighted_GCN_GNG, f1_weighted_GAT_GNG], \"x-\", label=\"GNG\")\n",
        "plt.plot([f1_weighted_GCN_SMP_R, f1_weighted_GAT_SMP_R], \"x-\", label=\"SMP_R\")\n",
        "\n",
        "# Add x-ticks for GCN and GAT\n",
        "plt.xticks([0, 1], [\"GCN\", \"GAT\"])\n",
        "plt.ylabel(\"F1 Score (Weighted)\")\n",
        "plt.title(\"Weighted F1 Comparison\")\n",
        "plt.legend()\n",
        "plt.legend(title=\"Region\")  # Show line labels\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"f1_comparison.png\", dpi=300, bbox_inches='tight')  # Save as PNG\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Label Propagation Accuracy: 0.7771\n"
          ]
        }
      ],
      "source": [
        "from torch_geometric.nn.models import LabelPropagation\n",
        "\n",
        "lp = LabelPropagation(num_layers=10, alpha=0.9)\n",
        "\n",
        "# Run label propagation\n",
        "y_pred = lp(graph.y, graph.edge_index, graph.train_mask)\n",
        "\n",
        "# Convert soft outputs to class predictions\n",
        "y_pred_labels = y_pred.argmax(dim=1)\n",
        "\n",
        "# Compute accuracy on test set\n",
        "acc = (y_pred_labels[graph.test_mask] == graph.y[graph.test_mask]).float().mean()\n",
        "print(f\"Label Propagation Accuracy: {acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Message passing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "def simple_gcn(x, edge_index, weight):\n",
        "    N = x.size(0)  # Number of nodes\n",
        "\n",
        "    # Add self-loops\n",
        "    loop_index = torch.arange(N)\n",
        "    self_loops = loop_index.unsqueeze(0).repeat(2, 1)\n",
        "    edge_index = torch.cat([edge_index, self_loops], dim=1)\n",
        "\n",
        "    row, col = edge_index\n",
        "    deg = torch.bincount(row, minlength=N).float()\n",
        "    deg_inv_sqrt = deg.pow(-0.5)\n",
        "    norm = (deg_inv_sqrt[row] * deg_inv_sqrt[col])\n",
        "\n",
        "    # Linear transformation\n",
        "    x = x @ weight\n",
        "\n",
        "    # Initialize output\n",
        "    out = torch.zeros_like(x)\n",
        "\n",
        "    # Manual message passing\n",
        "    for i in range(edge_index.size(1)):\n",
        "        src = row[i]\n",
        "        dst = col[i]\n",
        "        out[dst] += norm[i] * x[src]\n",
        "\n",
        "    return torch.relu(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch_geometric.data import Data\n",
        "\n",
        "# Inputs\n",
        "x = torch.tensor([\n",
        "    [1.0, 0.0],\n",
        "    [0.0, 1.0],\n",
        "    [1.0, 1.0],\n",
        "    [0.0, 0.0]\n",
        "])\n",
        "\n",
        "edge_index = torch.tensor([\n",
        "    [0, 1, 1, 1, 2, 3],\n",
        "    [1, 0, 2, 3, 1, 1]\n",
        "], dtype=torch.long)\n",
        "\n",
        "# Graph object for PyG\n",
        "data = Data(x=x, edge_index=edge_index)\n",
        "\n",
        "# Create PyG GCNConv layer\n",
        "gcn = GCNConv(in_channels=2, out_channels=3, bias=False)\n",
        "\n",
        "# Set the weights to match\n",
        "W = torch.tensor([\n",
        "    [0.1, 0.2, 0.3],\n",
        "    [0.4, 0.5, 0.6]\n",
        "], dtype=torch.float)\n",
        "\n",
        "with torch.no_grad():\n",
        "    gcn.lin.weight.copy_(W.T)\n",
        "\n",
        "# Apply PyG GCN\n",
        "out_pyg = gcn(data.x, data.edge_index)\n",
        "out_pyg = torch.relu(out_pyg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Custom GCN Output:\n",
            " tensor([[0.1914, 0.2768, 0.3621],\n",
            "        [0.3121, 0.4432, 0.5743],\n",
            "        [0.3914, 0.5268, 0.6621],\n",
            "        [0.1414, 0.1768, 0.2121]])\n",
            "\n",
            "PyG GCNConv Output:\n",
            " tensor([[0.1914, 0.2768, 0.3621],\n",
            "        [0.3121, 0.4432, 0.5743],\n",
            "        [0.3914, 0.5268, 0.6621],\n",
            "        [0.1414, 0.1768, 0.2121]], grad_fn=<ReluBackward0>)\n",
            "\n",
            "Are they close? True\n"
          ]
        }
      ],
      "source": [
        "# Run custom GCN\n",
        "out_custom = simple_gcn(x, edge_index, W)\n",
        "\n",
        "# Compare\n",
        "print(\"Custom GCN Output:\\n\", out_custom)\n",
        "print(\"\\nPyG GCNConv Output:\\n\", out_pyg)\n",
        "\n",
        "print(\"\\nAre they close?\", torch.allclose(out_custom, out_pyg, atol=1e-5))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
